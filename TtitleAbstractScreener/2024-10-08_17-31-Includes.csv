;title;authors;year;publicationtitle;citationcount;doi;abstract;population;intervention;comparison;outcomes;context;checklater;include;other;backtracking
2;Towards an Internet of Things Tangible Program Environment Supported by Indigenous African Artefacts;Smith, Andrew Cyrus and Dlodlo, Nomusa and Jere, Nobert;2016;Proceedings of the First African Conference on Human Computer Interaction;0;10.1145/2998581.2998599;According to its advocates, the Internet of Things holds great promise. Great strides have been made to address its security and standardise communication protocols for data exchange in this potentially unlimited network of connected things. However, a dimension that has not yet been adequately addressed is the human component and specifically how the individual selects personal preferences and expresses rules that direct the Internet of Things' behaviour to meet the individual's needs. We propose an approach that requires neither computer literacy nor fine motor skills as are often associated with computer-based configuration mechanisms. By combining certain Gestalt principles of human perception with handcrafted artefacts, we show how a tangible programming environment could be realised with which the Internet of Things can be configured to suit the individual.;0;0;0;0;0;0;1;0;0
8;ARgo: animate everyday object in augmented reality;Park, Byung-Hwa and Oh, Se-Young;2014;Proceedings of the 11th Conference on Advances in Computer Entertainment Technology;0;10.1145/2663806.2663865;We present ARgo(Augmented Reality tamaGOchi), a system for animating an everyday object on a smart device. We focus to interact with every everyday object on the smart device augmented reality(AR) through animating them with animating and gaming contents. We developed a Single-Point-Tap gesture event based object tracking algorithm using a GrabCut, TLD and CamShift tracking hybrid system for a markerless mobile AR application. To make the object seem to have an inherent facial expression, we use Poisson image edit, which is a seamless blending approach. To instill 'life' to an everyday object, we adopted the Tamagotchi storytelling concept. For multi-platform game application purposes and computer vision programming, the AR programming environment was built using the Cocos2d-x game engine and OpenCV. We show the use of our system to animate the everyday object in real environment.;0;0;0;0;0;0;1;0;0
10;Flexel: A Modular Floor Interface for Room-Scale Tactile Sensing;Yoshida, Takatoshi and Okazaki, Narin and Takaki, Ken and Hirose, Masaharu and Kitagawa, Shingo and Inami, Masahiko;2022;Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology;0;10.1145/3526113.3545699;Human environments are physically supported by floors, which prevent people and furniture from gravitational pull. Since our body motions continuously generate vibrations and loads that propagate into the ground, measurement of these expressive signals leads to unobtrusive activity sensing. In this study, we present Flexel, a modular floor interface for room-scale tactile sensing. By paving a room with floor interfaces, our system can immediately begin to infer touch locations, track user locations, recognize foot gestures, and detect object locations. Through a series of exploratory studies, we determined the preferable hardware design that adheres to construction conventions, as well as the optimal sensor density that mediates the trade-off between cost and performance. We summarize our findings into design guidelines that are generalizable to other floor interfaces. Finally, we provide example applications for room-scale tactile sensing enabled by our Flexel system.;0;0;0;0;0;0;1;0;0
11;Wall++: Room-Scale Interactive and Context-Aware Sensing;Zhang, Yang and Yang, Chouchang (Jack) and Hudson, Scott E. and Harrison, Chris and Sample, Alanson;2018;Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems;0;10.1145/3173574.3173847;Human environments are typified by walls, homes, offices, schools, museums, hospitals and pretty much every indoor context one can imagine has walls. In many cases, they make up a majority of readily accessible indoor surface area, and yet they are static their primary function is to be a wall, separating spaces and hiding infrastructure. We present Wall++, a low-cost sensing approach that allows walls to become a smart infrastructure. Instead of merely separating spaces, walls can now enhance rooms with sensing and interactivity. Our wall treatment and sensing hardware can track users' touch and gestures, as well as estimate body pose if they are close. By capturing airborne electromagnetic noise, we can also detect what appliances are active and where they are located. Through a series of evaluations, we demonstrate Wall++ can enable robust room-scale interactive and context-aware applications.;0;0;0;0;0;0;1;0;0
13;MovableBag: Substitutional Robot for Enhancing Immersive Boxing Training with Encountered-Type Haptic;Mendez S., Luis Andres and Ng, Ho Yin and Lim, Zin Yin and Lu, Yi-Jie and Han, Ping-Hsuan;2022;SIGGRAPH Asia 2022 XR;0;10.1145/3550472.3558406;Boxing is a combat sport that involves fierce movement and close body contact between two boxers. In addition to the boxer, the coach is also essential for training. The coach could be injured or too tired to continue consecutive training sessions for many boxers. Thus, we present MovableBag, a substitutional robot that provides room-scale mobility and the ability to carry sport props with an easier tracking setup to simulate the boxing opponent or coach in virtual reality with encountered-type haptic feedback. In addition, we integrate our system with a wireless VR headset and develop a VR boxing application to show the capacity of the device and inform future studies.;0;0;0;0;0;0;1;0;0
14;Tangiball: Dynamic Embodied Tangible Interaction with a Ball in Virtual Reality;Bozgeyikli, Lila and Bozgeyikli, Evren;2019;Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion;0;10.1145/3301019.3323904;Tangible interaction in virtual reality (VR) is known to offer several benefits. Although using tangible objects as controllers in VR is common, incorporating real-life tangible objects for dynamic embodied interactions is not a well-researched area. To that end, we propose a tangible soccer game system (?Tangiball') in room-scale VR, in which users interact with a tangible ball with their feet in real-time by only seeing its virtual representation inside a head mounted display. Tangiball includes a custom-built transparent tangible ball, inside which motion tracking markers are placed using a custom 3D printed attachment. The uniqueness of the Tangiball lies in the dynamic embodied tangible VR interaction it offers. This paper includes the design decisions and iterations, system details and lessons learned. The results of the pilot testing are promising, as users engaged with the Tangiball effortlessly and intuitively and mentioned that it was playful.;0;0;0;0;0;0;1;0;0
17;Ubi-TOUCH: Ubiquitous Tangible Object Utilization through Consistent Hand-object interaction in Augmented Reality;Jain, Rahul and Shi, Jingyu and Duan, Runlin and Zhu, Zhengzhe and Qian, Xun and Ramani, Karthik;2023;Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology;0;10.1145/3586183.3606793;Utilizing everyday objects as tangible proxies for Augmented Reality (AR) provides users with haptic feedback while interacting with virtual objects. Yet, existing methods focus on the attributes of the objects, constraining the possible proxies and yielding inconsistency in user experience. Therefore, we propose Ubi-TOUCH, an AR system that assists users in seeking a wider range of tangible proxies for AR applications based on the hand-object interaction (HOI) they desire. Given the target interaction with a virtual object, the system scans the users’ vicinity and recommends object proxies with similar interactions. Upon user selection, the system simultaneously tracks and maps users’ physical HOI to the virtual HOI, adaptively optimizing object 6 DoF and the hand gesture to provide consistency between the interactions. We showcase promising use cases of Ubi-TOUCH, such as remote tutorials, AR gaming, and Smart Home control. Finally, we evaluate the performance and usability of Ubi-TOUCH with a user study.;0;0;0;0;0;0;1;0;0
18;PAIR: Phone as an Augmented Immersive Reality Controller;Unlu, Arda Ege and Xiao, Robert;2021;Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology;0;10.1145/3489849.3489878;Immersive head-mounted augmented reality allows users to overlay 3D digital content on a user’s view of the world. Current-generation devices primarily support interaction modalities such as gesture, gaze and voice, which are readily available to most users yet lack precision and tactility, rendering them fatiguing for extended interactions. We propose using smartphones, which are also readily available, as companion devices complementing existing AR interaction modalities. We leverage user familiarity with smartphone interactions, coupled with their support for precise, tactile touch input, to unlock a broad range of interaction techniques and applications - for instance, turning the phone into an interior design palette, touch-enabled catapult or AR-rendered sword. We describe a prototype implementation of our interaction techniques using an off-the-shelf AR headset and smartphone, demonstrate applications, and report on the results of a positional accuracy study.;0;0;0;0;0;0;1;0;0
20;Discovery table exploring the design of tangible and ubiquitous technology for learning in preparatory classrooms;Bod\'{e}n, Marie and Dekker, Andrew and Viller, Stephen;2011;Proceedings of the 23rd Australian Computer-Human Interaction Conference;0;10.1145/2071536.2071543;In this paper we investigate how technologies can be designed to support learning in preparatory classrooms by augmenting existing learning objects. By following an iterative interaction design process of working with teachers and children within the classroom environment, we can better design technologies through the augmentation (rather than replacement) of existing learning activities. Our case study -- the Discovery Table -- uses a variety of technologies to allow everyday plastic symbols of letters and numbers to be placed on a technology augmented table in order to provide visual, audible and tangible feedback to the children. Discovery Table demonstrates a first step towards more fundamental work towards successful design for tangible learning.;0;0;0;0;0;0;1;0;0
21;FarOut Touch: Extending the Range of ad hoc Touch Sensing with Depth Cameras;Shen, Vivian and Spann, James and Harrison, Chris;2021;Proceedings of the 2021 ACM Symposium on Spatial User Interaction;0;10.1145/3485279.3485281;The ability to co-opt everyday surfaces for touch interactivity has been an area of HCI research for several decades. Ideally, a sensor operating in a device (such as a smart speaker) would be able to enable a whole room with touch sensing capabilities. Such a system could allow for software-defined light switches on walls, gestural input on countertops, and in general, more digitally flexible environments. While advances in depth sensors and computer vision have led to step-function improvements in the past, progress has slowed in recent years. We surveyed the literature and found that the very best ad hoc touch sensing systems are able to operate at ranges up to around 1.5 m. This limited range means that sensors must be carefully positioned in an environment to enable specific surfaces for interaction. In this research, we set ourselves the goal of doubling the sensing range of the current state of the art system. To achieve this goal, we leveraged an interesting finger ”denting” phenomena and adopted a marginal gains philosophy when developing our full stack. When put together, these many small improvements compound and yield a significant stride in performance. At 3 m range, our system offers a spatial accuracy of 0.98 cm with a touch segmentation accuracy of 96.1%, in line with prior systems operating at less than half the range. While more work remains to be done to achieve true room-scale ubiquity, we believe our system constitutes a useful advance over prior work.;0;0;0;0;0;0;1;0;0
35;BoldMove: Enabling IoT Device Control on Ubiquitous Touch Interfaces by Semantic Mapping and Sequential Selection;Zhang, Tengxiang and Zeng, Xin and Zhang, Yinshuai and Jiang, Xin and Xu, Xuhai and Dey, Anind K and Chen, Yiqiang;2022;Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems;0;10.1145/3491101.3519805;Recent advances in ultra-low-power ubiquitous touch interfaces make touch inputs possible anytime, anywhere. However, their functions are usually pre-determined, i.e., one button is only associated with one fixed function. BoldMove enables spontaneous and efficient association of touch inputs and IoT device functions with semantic-based function filtering and a wait-confirm sequential selection strategy. In this way, such touch interfaces become ubiquitous IoT device controllers. We proposed the semantic-based IoT function filtering to improve control efficiency, then designed the sequential selection mechanism for interfaces with constrained input and output resources. We implemented BoldMove on a custom-built touch interface with capacitive button inputs and a smartwatch display. We then conducted a user study to determine the design parameters for the sequential selection method. At last, we validated that BoldMove only takes 3.25 seconds to complete a selection task if the target function appears within the Top-3 displayed item. Even if the assumption is relaxed to Top-10, BoldMove is still estimated to be more efficient than the conventional selection method with device-based filtering and menu-navigated selection.;0;0;0;0;0;0;1;0;0
41;Ubiquitous drums: a tangible, wearable musical interface;Smus, Boris and Gross, Mark D.;2010;CHI '10 Extended Abstracts on Human Factors in Computing Systems;0;10.1145/1753846.1754094;Drummers and non-drummers alike can often be seen making percussive gestures on their chests, knees and feet. Ubiquitous Drums enhances this experience by providing musical feedback for these and other gestures. This paper describes the implementation and evolution of this tangible, wearable musical instrument.;0;0;0;0;0;0;1;0;0
50;Augmented museum experience through Tangible Narrative;Ciotoli, Luca and Alinam, Morteza and Torre, Ilaria;2022;Proceedings of the 20th International Conference on Mobile and Ubiquitous Multimedia;0;10.1145/3490632.3497837;"This paper is about an interactive and tangible storytelling installation based on the principle of ubiquitous museum. The installation is designed to focus children’s attention on the sailing practice in the Medieval age by engaging children in an experience where they interact with several ancient navigation tools, equipped with sensors and communication capabilities. The networked infrastructure is combined with the approach of tangible narratives to augment the museum experience. We conducted a pilot study with three different groups of participants in order to investigate the interaction with the augmented ancient navigation tools. We used quantitative and qualitative data to get feedback on participants’ engagement while interacting with diegetic and non-diegetic objects and on factors that can impact engagement. Even though preliminary, the results can be useful to designers of ubiquitous museum installations to know possible risks and factors to take into account at design time. [disagreement; is it really an interactive object?]";0;0;0;0;0;0;1;0;0
57;Technology Individuation: The Foibles of Augmented Everyday Objects;Ambe, Aloha Hufana and Brereton, Margot and Soro, Alessandro and Roe, Paul;2017;Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems;0;10.1145/3025453.3025770;This paper presents the concept of technology individuation and explores its role in design. Individuation expresses how, over time, a technology becomes personal and intimate, unique in purpose, orchestrated in place, and how people eventually come to rely on it to sustain connection with others. We articulate this concept as a critical vantage point for designing augmented everyday objects and the Internet of Things. Individuation foregrounds aspects of habituation, routines and arrangements that through everyday practices reveal unique meaning, reflect self-identity and support agency.The concept is illustrated through three long term case studies of technology in use, involving tangible and embodied interaction with devices that afford communication, monitoring, and awareness in the home setting. The cases are analysed using Hornecker and Buur's Tangible Interaction Framework. We further extend upon this framework to better reveal the role played by personal values, history of use, and arrangements, as they develop over time in the home setting, in shaping tangible and embodied interaction with individuated technologies.;0;0;0;0;0;0;1;0;0
59;reMi: Translating Ambient Sounds of Moment into Tangible and Shareable Memories through Animated Paper;Choi, Kyung Yun and Shinsato, Darle and Zhang, Shane and Nakagaki, Ken and Ishii, Hiroshi;2018;Adjunct Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology;0;10.1145/3266037.3266109;"We present a tangible memory notebook--reMi--that records the ambient sounds and translates them into a tangible and shareable memory using animated paper. The paper replays the recorded sounds and deforms its shape to generate synchronized motions with the sounds. Computer-mediated communication interfaces have allowed us to share, record and recall memories easily through visual records. However, those digital visual-cues that are trapped behind the device's 2D screen are not the only means to recall a memory we experienced with more than the sense of vision. To develop a new way to store, recall and share a memory, we investigate how tangible motion of a paper that represents sound can enhance the ""reminiscence"". [disagreement; notebook vs device?]";0;0;0;0;0;0;1;0;0
64;Evaluating elementary student interaction with ubiquitous touch projection technology;Diesburg, Sarah M and Feldhaus, C. Adam and Oswald, Coved and Boudreau, Cole and Brown, Beau;2018;Proceedings of the 17th ACM Conference on Interaction Design and Children;0;10.1145/3202185.3202757;Ubiquitous touch projection technology is entering the market and allows touch screens to be projected onto any smooth surface at a lower cost. We created a mathematics manipulative application for a low-cost, ubiquitous touch projection system, and we conducted an observational study with 18 third-grade elementary students. Through our analysis of both video and system data, we identify common student interaction problems, evaluate how those interaction problems affect usability, engagement, and pair-work, and we make practical suggestions to aid future developers, educators, and students in using such a platform.;0;0;0;0;0;0;1;0;0
65;Tangible Globes for Data Visualisation in Augmented Reality;Satriadi, Kadek Ananta and Smiley, Jim and Ens, Barrett and Cordeil, Maxime and Czauderna, Tobias and Lee, Benjamin and Yang, Ying and Dwyer, Tim and Jenny, Bernhard;2022;Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems;0;10.1145/3491102.3517715;"Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the “tangible-virtual interplay” of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR. [disagremeent; stand? how complete / proxy]";0;0;0;0;0;0;1;0;0
67;Ubi Edge: Authoring Edge-Based Opportunistic Tangible User Interfaces in Augmented Reality;He, Fengming and Hu, Xiyun and Shi, Jingyu and Qian, Xun and Wang, Tianyi and Ramani, Karthik;2023;Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems;0;10.1145/3544548.3580704;Edges are one of the most ubiquitous geometric features of physical objects. They provide accurate haptic feedback and easy-to-track features for camera systems, making them an ideal basis for Tangible User Interfaces (TUI) in Augmented Reality (AR). We introduce Ubi Edge, an AR authoring tool that allows end-users to customize edges on daily objects as TUI inputs to control varied digital functions. We develop an integrated AR-device and an integrated vision-based detection pipeline that can track 3D edges and detect the touch interaction between fingers and edges. Leveraging the spatial-awareness of AR, users can simply select an edge by sliding fingers along it and then make the edge interactive by connecting it to various digital functions. We demonstrate four use cases including multi-function controllers, smart homes, games, and TUI-based tutorials. We also evaluated and proved our system’s usability through a two-session user study, where qualitative and quantitative results are positive.;0;0;0;0;0;0;1;0;0
71;Tangible Version Control: Exploring a Physical Object’s Alternative Versions;Letter, Maximilian and Wolf, Katrin;2022;Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems;0;10.1145/3491101.3519686;In iterative physical object creation, only the latest design state is manifested in the physical artifact, while information about previous versions are lost. This makes it challenging to keep track of changes and developments in iterative physical design. In this paper, we propose the concept of Tangible Version Control (TVC), inspired by the visualizations of traditional version control systems. In TVC, the real-world artifact itself is used for exploring its alternative versions in physical space, while comparisons to an alternative version are displayed seamlessly on the artifact with the use of augmented reality. Our implementation of TVC includes three different comparison modes, namely SideBySide, Overlay, and Differences. Furthermore, we discuss the anticipated use, opportunities, and challenges of using TVC in the future for individual users as well as for asynchronous collaborative work.;0;0;0;0;0;0;1;0;0
73;Digital Augmentation of Historical Objects Through Tangible Interaction;Not, Elena and Cavada, Dario and Maule, Stefano and Pisetti, Anna and Venturini, Adriano;2019;J. Comput. Cult. Herit.;0;10.1145/3297764;"The technological advances brought about by the Internet of Things enable new opportunities for a more direct interaction among users, objects, and places. This is an extremely valuable innovation for the cultural heritage sector, as it allows a more transparent use of technology in the digital augmentation of museums and cultural heritage sites. The possibility to augment physical objects with sensors detecting when they are moved and manipulated enables scenarios where descriptive information about objects is presented to users at the very exact time they are looking at them, stimulating engagement. This article describes a collaborative research effort among cultural heritage professionals, human--computer interaction experts, and developers that was aimed at investigating the goals and constraints curators consider for a physical encounter between visitors and historic relics. In a case study, we co-designed an interactive plinth centred on tangible interaction and evaluated the impact on the user experience of combining digital information with a hands-on experience of relics of World War I. Our findings show that visitors value this type of tangible interaction with collection objects positively, as it allows the discovery of details and the learning of aspects that normally go unnoticed. The synergy between physical and digital aspects stimulates empathy with the original users of the object and fosters social interaction. [disagreement; amouint of interaction]";0;0;0;0;0;0;1;0;0
74;Milky: on-product app for emotional product to human interactions;Deru, Matthieu and Bergweiler, Simon;2013;Proceedings of the 15th International Conference on Human-Computer Interaction with Mobile Devices and Services;0;10.1145/2493190.2494423;"In this paper we present a new way of emotional interaction with products. Based on the rapid prototyping Microsoft Gadgeteer platform, we concretized our vision of an on-product app by implementing an anthropomorphic intelligent milk carton. The purpose of this realization is to give customers a better view of a product's life-cycle. This realization also demonstrates that the frontier between pure mobile applications development and the creation of tangible objects is very thin and opens new way to integrate the Internet of Things over an anthropomorphic user interface, thus leading to a new product to human interaction form. [disagreement; same thing or addon; eo now or in the future]";0;0;0;0;0;0;1;0;0
78;Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching;Monteiro, Kyzyl and Vatsal, Ritik and Chulpongsatorn, Neil and Parnami, Aman and Suzuki, Ryo;2023;Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems;0;10.1145/3544548.3581449;This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.;0;0;0;0;0;0;1;0;0
85;Implicit interaction with daily objects: applications and issues;Fujinami, Kaori;2009;Proceedings of the 3rd International Universal Communication Symposium;0;10.1145/1667780.1667813;This paper describes augmentation of daily objects as a mean to interact with a ubiquitous/pervasive computing environment. A daily object employs a context-aware capability, where a user's specific context is captured implicitly and naturally by sensors from its original usage because such an everyday object has inherent roles and functionalities. Also, information is presented naturally and effectively during the utilization. A user does not need to learn how to get information, which fills the gap between a user and a complex ubiquitous/pervasive computing environment.In this paper, some projects on augmenting daily objects are presented, where possible applications and a technique to complement a missing piece of context that is obtained only from an instrumental object are presented. Also, we propose to assure a sensor placement for reliable sensing by a daily object.;0;0;0;0;0;0;1;0;0
88;Augmented reality and tangible user interfaces integration for enhancing the user experience;Rodrigues, F\'{a}bio and Sato, Fernando and Botega, Leonardo and Oliveira, Allan;2012;Proceedings of the 11th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry;0;10.1145/2407516.2407536;The integration of post-wimp computer interfaces arises as an alternative to meet individual limitations of each one, considering both interaction components and feedbacks to users. Tangible interfaces can present restrictions referring to physical space on tabletop architectures, which limits the manipulation of objects and deprecates the interactive process. Hence, this paper proposes the integration of techniques of mobile Augmented Reality with tabletop tangible architecture for blending real and virtual components on its surface, aiming to make the interactive process richer, seamless and more complete.;0;0;0;0;0;0;1;0;0
93;"""...a load of ould boxology!""";Ferris, Kieran and Bannon, Liam;2002;Proceedings of the 4th Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques;0;10.1145/778712.778722;"This paper documents the design process for an augmented children's play environment centred on that most ubiquitous and simple of objects, the cardboard box. The purpose of the exercise is to show how computer technology can be used in innovative ways to stimulate discovery, play and adventure among children. Our starting point was a dissatisfaction with current computer technology as it is presented to children, which, all to often in our view, focuses inappropriately on the computer per se as a fetishized object. Shifting the focus of attention from the Graphical User Interface (GUI) to familiar objects, and children's interactions around and through these augmented objects, results in the computer becoming a facilitator of exploration and learning. The paper documents the journey from initial design concept, through a number of prototype implementations, to the final implementation. Each design iteration was triggered by observation of use of the prototypes, and reflection on that use, and on new design possibilities. By augmenting an everyday artefact, namely the standard cardboard box, we have created a simple yet powerful interactive environment that, judging from the experience of our ""users"", has achieved its goal of stirring children's imagination.";0;0;0;0;0;0;1;0;0
94;Augmented tangible interface components and image based interactions;Kanev, Kamen;2012;Proceedings of the 13th International Conference on Computer Systems and Technologies;0;10.1145/2383276.2383280;"This work focuses on digital object and environmental enhancements through durable surface and undersurface encoding by printing and laser marking. While resulting augmented physical objects and environmental surfaces preserve their usual appearance, they become easily recognizable and tractable by optical means. Based on such encodings novel interaction mechanisms are being developed and concrete works on interior pattern design and physical interface component construction are presented along with their applications in the creation and employment of tangible interfaces and image-based interaction. [disagreement; is it really something]";0;0;0;0;0;0;1;0;0
96;BrickStARt: Enabling In-situ Design and Tangible Exploration for Personal Fabrication using Mixed Reality;Stemasov, Evgeny and Hohn, Jessica and Cordts, Maurice and Schikorr, Anja and Rukzio, Enrico and Gugenheimer, Jan;2023;Proc. ACM Hum.-Comput. Interact.;0;10.1145/3626465;"3D-printers enable end-users to design and fabricate unique physical artifacts but maintain an increased entry barrier and friction. End users must design tangible artifacts through intangible media away from the main problem space (ex-situ) and transfer spatial requirements to an abstract software environment. To allow users to evaluate dimensions, balance, or fit early and in-situ, we developed BrickStARt, a design tool using tangible construction blocks paired with a mixed-reality headset. Users assemble a physical block model at the envisioned location of the fabricated artifact.  
Designs can be tested tangibly, refined, and digitally post-processed, remaining continuously in-situ. We implemented BrickStARt using a Magic Leap headset and present walkthroughs, highlighting novel interactions for 3D-design. In a user study (n=16), first-time 3D-modelers succeeded more often using BrickStARt than Tinkercad. Our results suggest that BrickStARt provides an accessible and explorative process while facilitating quick, tangible design iterations that allow users to detect physics-related issues (e.g., clearance) early on.";0;0;0;0;0;0;1;0;0
104;Wizard of Props: Mixed Reality Prototyping with Physical Props to Design Responsive Environments;Zhang, Yuzhen and Han, Ruixiang and Zhou, Ran and Gyory, Peter and Zheng, Clement and Shih, Patrick C. and Do, Ellen Yi-Luen and Jung, Malte F and Ju, Wendy and Leithinger, Daniel;2024;Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction;0;10.1145/3623509.3633395;Driven by the vision of future responsive environments, where everyday surroundings can perceive human behaviors and respond through intelligent robotic actuation, we propose Wizard of Props (WoP): a human-centered design workflow for creating expressive, implicit, and meaningful interactions. This collaborative experience prototyping approach integrates full-scale physical props with Mixed Reality (MR) to support ideation, prototyping, and rapid testing of responsive environments. We present two design explorations that showcase our investigations of diverse design solutions based on varying technology resources, contextual considerations, and target audiences. Design Exploration One focuses on mixed environment building, where we observe fluid prototyping methods. In Design Exploration Two, we explore how novice designers approach WoP, and illustrate their design ideas and behaviors. Our findings reveal that WoP complements conventional design methods, enabling intuitive body-storming, supporting flexible prototyping fidelity, and fostering expressive environment-human interactions through in-situ improvisational performance.;0;0;0;0;0;0;1;0;0
105;Augmented Reality with Tangible Auto-Fabricated Models for Molecular Biology Applications;Gillet, Alexandre and Sanner, Michel and Stoffler, Daniel and Goodsell, David and Olson, Arthur;2004;Proceedings of the Conference on Visualization '04;0;10.1109/VISUAL.2004.7;"The evolving technology of computer auto-fabrication (""3-D printing"") now makes it possible to produce physical models for complex biological molecules and assemblies. We report on an application that demonstrates the use of auto-fabricated tangible models and augmented reality for research and education in molecular biology, and for enhancing the scientific environment for collaboration and exploration. We have adapted an augmented reality system to allows virtual 3-D representations (generated by the Python Molecular Viewer) to be overlaid onto a tangible molecular model. Users can easily change the overlaid information, switching between different representations of the molecule, displays of molecular properties such as electrostatics, or dynamic information. The physical model provides a powerful, intuitive interface for manipulating the computer models, streamlining the interface between human intent, the physical model, and the computational activity. [disagreement; works with any model or specific model?]";0;0;0;0;0;0;1;0;0
106;HeatGoggles: Enabling Ubiquitous Touch Input through Head-Mounted Devices using Thermal Imaging;Faltaous, Sarah and Wittpoth, Mark and Abdelrahman, Yomna and Schneegass, Stefan;2022;Proceedings of the 21st International Conference on Mobile and Ubiquitous Multimedia;0;10.1145/3568444.3570597;Displays are a ubiquitous part of our everyday life. Digital, as well as paper-based displays, are used on various occasions to present us with different information. While users cannot interact with many of these displays yet, they could benefit from making them interactive. Research shows that thermal cameras can be exploited to make surfaces interactive by sensing touchpoints based on the heat signature and residue. However, most of the research done uses a stationary setup. This work proposes HeatGoggles, a wearable gadget detecting touch input on arbitrary surfaces using thermal imaging. We report on its design, prototypical implementation, and first evaluation. We show that it can be used on non-touch-enabled displays as well as other traditional information displays.;0;0;0;0;0;0;1;0;0
119;Shiva's Rangoli: Tangible Storytelling through Diegetic Interfaces in Ambient Environments;Gupta, Saumya and Tanenbaum, Theresa Jean and Tanenbaum, Karen;2019;Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction;0;10.1145/3294109.3295635;This paper describes the underlying motivation, creation process, and evaluation outcomes of Shiva's Rangoli, a tangible storytelling installation that allows readers to impact the emotional tone of a narrative by sculpting the ambience of their space. Readers interact with a tangible interface that acts as a boundary object between the reader and the fictional world. We discuss how these kinds of interfaces can engage readers to feel like they are a part of the story, endow them with responsibility, and blur the line between real and fictional worlds.;0;0;0;0;0;0;1;0;0
127;Using physical objects to enable enriched video communication;Nyberg, Marcus and Norlin, Cristian and Gomez, Peter;2011;Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services;0;10.1145/2037373.2037454;"This paper describes an exploratory concept for how video communication can address the potential collaboration opportunities (and challenges) that arise with an emerging networked society in which the ""material"" to be used in the collaboration no longer is restrained to simple presentations, but can include services, Internet enabled objects, and many other types of systems and features. The concept illustrates how tangible objects can be utilized as props for the interaction and collaboration, and as access points to services, functionality and information. The findings from a qualitative user study suggest that this contributes to creating a form of collaboration in which technology is less visible and the actual meeting between humans becomes more significant. The user study also showed the importance of security and trust for such a system to work. [disagreement; speculative; id vs key]";0;0;0;0;0;0;1;0;0
129;An augmented workplace for enabling user-defined tangibles;Funk, Markus and Korn, Oliver and Schmidt, Albrecht;2014;CHI '14 Extended Abstracts on Human Factors in Computing Systems;0;10.1145/2559206.2581142;In this work, we introduce a novel setup for an augmented workplace, which allows for defining and interacting with user-defined tangibles. State-of-the-art tangible user interface systems equip both the underlying surface and the tangible control with sensors or markers. At the workplace, having one unique tangible for each available action results in confusion. Furthermore, tangible controls mix with regular objects and induce a messy desk. Therefore, we introduce the concept of user-defined tangibles, which enable a spontaneous binding between physical objects and digital functions. With user-defined tangibles the need for specially designed tangible controls disappears and each physical object on the augmented workplace can be turned into a tangible control. We introduce a prototypical system and outline an interaction concept.;0;0;0;0;0;0;1;0;0
132;Multi-Touch Querying on Data Physicalizations in Immersive AR;Herman, Bridger and Omdal, Maxwell and Zeller, Stephanie and Richter, Clara A. and Samsel, Francesca and Abram, Greg and Keefe, Daniel F.;2021;Proc. ACM Hum.-Comput. Interact.;0;10.1145/3488542;"Data physicalizations (3D printed terrain models, anatomical scans, or even abstract data) can naturally engage both the visual and haptic senses in ways that are difficult or impossible to do with traditional planar touch screens and even immersive digital displays. Yet, the rigid 3D physicalizations produced with today's most common 3D printers are fundamentally limited for data exploration and querying tasks that require dynamic input (e.g., touch sensing) and output (e.g., animation), functions that are easily handled with digital displays. We introduce a novel style of hybrid virtual + physical visualization designed specifically to support interactive data exploration tasks. Working toward a ""best of both worlds"" solution, our approach fuses immersive AR, physical 3D data printouts, and touch sensing through the physicalization. We demonstrate that this solution can support three of the most common spatial data querying interactions used in scientific visualization (streamline seeding, dynamic cutting places, and world-in-miniature visualization). Finally, we present quantitative performance data and describe a first application to exploratory visualization of an actively studied supercomputer climate simulation data with feedback from domain scientists. [disagreement; art]";0;0;0;0;0;0;1;0;0
133;Opportunistic Interfaces for Augmented Reality: Transforming Everyday Objects into Tangible 6DoF Interfaces Using Ad hoc UI;Du, Ruofei and Olwal, Alex and Le Goc, Mathieu and Wu, Shengzhi and Tang, Danhang and Zhang, Yinda and Zhang, Jun and Tan, David Joseph and Tombari, Federico and Kim, David;2022;Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems;0;10.1145/3491101.3519911;Real-time environmental tracking has become a fundamental capability in modern mobile phones and AR/VR devices. However, it only allows user interfaces to be anchored at a static location. Although fiducial and natural-feature tracking overlays interfaces with specific visual features, they typically require developers to define the pattern before deployment. In this paper, we introduce opportunistic interfaces to grant users complete freedom to summon virtual interfaces on everyday objects via voice commands or tapping gestures. We present the workflow and technical details of Ad hoc UI (AhUI), a prototyping toolkit to empower users to turn everyday objects into opportunistic interfaces on the fly. We showcase a set of demos with real-time tracking, voice activation, 6DoF interactions, and mid-air gestures and prospect the future of opportunistic interfaces.;0;0;0;0;0;0;1;0;0
137;IoT Stickers: Enabling Lightweight Modification of Everyday Objects;Williams, Kristin;2020;Adjunct Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology;0;10.1145/3379350.3415807;Internet-of-Things (IoT) devices promise to enhance even the most mundane of objects with computational properties by seamlessly coupling the virtual world to the physical. However, IoT's associated costs and cumbersome setup limits its extension to many everyday tasks and objects, such as those in the home. To address these issues, my dissertation work will enable IoT Stickers'a book of inexpensive, battery-free sensors and composition patterns'to support customizing everyday objects with software and web services using stickers. Using RFID-based paper mechanisms, IoT Stickers integrates common sensors and web services with interactive stickers through a trigger-action architecture. This integration enables computational services to be tailored to everyday activities by setting parameters to be passed to the sticker's actions and composing the stickers together. Thus, IoT Stickers demonstrates a way to associate IoT services with a dramatically wider set of objects and tasks.;0;0;0;0;0;0;1;0;0
138;Using Everyday Objects as Props for Virtual Objects in First Person Augmented Reality Games: An Elicitation Study;Greenslade, Mac and Clark, Adrian and Lukosch, Stephan;2023;Proc. ACM Hum.-Comput. Interact.;0;10.1145/3611052;In this paper, we present an elicitation study which explores how people use common household objects as props to control virtual objects in augmented reality first-person perspective games. 24 participants were asked to select items from a range of common household objects to use as controllers for three different types of virtual object: a sword, shield, and crossbow. Participants completed short gameplay tasks using their selected items and rated the AR experience using the Augmented Reality Immersion (ARI) questionnaire. Results found no strong consensus linking any specific household object to any virtual object across our test group and, in addition, those who chose the most commonly selected object for each task did not have significantly higher scores on the ARI questionnaire compared to those who did not. A short post-experiment interview indicated a few key factors that were important to participants when selecting their household object, such as shape, size, grip feel and weight distribution. Based on our findings we recommend that developers provide the ability for users to choose which household objects to use as props based on the user's own preferences, and that they design intuitive ways for users to interact with virtual objects.;0;0;0;0;0;0;1;0;0
151;Tangible Immersive Trauma Simulation: Is Mixed Reality the next level of medical skills training?;Uhl, Jakob Carl and Schrom-Feiertag, Helmut and Regal, Georg and Gallhuber, Katja and Tscheligi, Manfred;2023;Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems;0;10.1145/3544548.3581292;In medical simulation training two approaches are currently rather disjunct: realistic manikins are used to teach physical skills and procedures and VR systems are used to train situation assessment and decision making. We propose a mixed reality approach, which allows trainees to use real tools and their hands when interacting with a physical manikin overlaid with a responsive virtual avatar. In close exchange with first responder organizations, we developed and evaluated an MR training scenario. In the scenario, users can talk to injured people in a car accident, assess the threat of the environment, and utilize real medical equipment. Participants experienced high levels of physical- and self-presence, increased stress levels, and reported a high technology acceptance. The proposed main requirements of first responders regarding haptic multi-sensory skill training in MR and the lessons learned from the workshop aim to guide the design of training solutions for medical training in MR.;0;0;0;0;0;0;1;0;0
155;PalimPost: information convergence using sticky notes;Bian, Li and Shilkrot, Roy;2011;Proceedings of the Second International Workshop on Web of Things;0;10.1145/1993966.1993984;In today's world, the digital information retrieval experience is inherently a sparse device-centric activity. Users rely on the ability of the currently used device to supply the requested information, in some disconnection from past activities on other devices. There is a growing need to develop new methods of connecting cross-context information retrieval sessions. We present PalimPost, a converged system for storing, searching, and sharing digital and physical world information using sticky notes and mobile devices. PalimPost extracts contextual cues from a user's physical environment and activities, and connects them to the user's digital world research. Subsequently, the system presents systematically categorized information that is relevant to the moment of interaction in a just-in-time manner. PalimPost uses physical sticky notes with embedded QR codes, as well as virtual sticky notes on mobile devices. The system incorporates Automatic Speech Recognition (ASR), Optical Character Recognition (OCR), and Natural Language Processing (NLP) techniques for understanding and categorizing the content.;0;0;0;0;0;0;1;0;0
159;Invisible connections: investigating older people's emotions and social relations around objects;Vaisutis, Kate and Brereton, Margot and Robertson, Toni and Vetere, Frank and Durick, Jeannette and Nansen, Bjorn and Buys, Laurie;2014;Proceedings of the SIGCHI Conference on Human Factors in Computing Systems;0;10.1145/2556288.2557314;The advent of the Internet of Things creates an interest in how people might interrelate through and with networks of internet enabled objects. With an emphasis on fostering social connection and physical activity among older people, this preliminary study investigated objects that people over the age of 65 years viewed as significant to them. We conducted contextual interviews in people's homes about their significant objects in order to understand the role of the objects in their lives, the extent to which they fostered emotional and social connections and physical activity, and how they might be augmented through internet connection.Discussion of significant objects generated considerable emotion in the participants. We identified objects of comfort and routine, objects that exhibited status, those that fostered independence and connection, and those that symbolized relationships with loved ones. These findings lead us to consider implications for the design of interconnected objects.;0;0;0;0;0;0;1;0;0
162;Expressing Intent: An Exploration of Rich Interactions;Ng, Rachel S. and Kandala, Raghavendra and Marie-Foley, Sarah and Lo, Dixon and Steenson, Molly Wright and Lee, Austin S.;2016;Proceedings of the TEI '16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction;0;10.1145/2839462.2856526;In this paper, we describe Expressing Intent - our initial exploration of rich interactions between human actors and three connected objects -- (1) a bookshelf that learns about taste, (2) a radio that determines mood, and (3) a window that augments visual reality. These objects interpret and express 'intent' in a multitude of ways within the context of a shared office space. Objects with intent, or animistic qualities, can evoke diverse reactions from human actors, depending on how they are designed. To investigate the effects of multiple human and non-human actors interacting with self-interest in mind, we deliberately designed each object with distinct needs and values that complement human behavior when placed in a shared office space. The resultant system of interactions involves cascading relations between object-object, human-object and object-human. Further, after our initial prototype, we discover prime areas in interaction design that warrant further exploration. Specifically, the implications of incorporating animism in object design, objects with needs and values independent of their users, and the implications of designing connected heterogeneous ecosystems (i.e. distinct but cooperative objects) vs. homogenous ecosystems (i.e. uniform and cooperative objects).;0;0;0;0;0;0;1;0;0
164;Opportunistic controls: leveraging natural affordances as tangible user interfaces for augmented reality;Henderson, Steven J. and Feiner, Steven;2008;Proceedings of the 2008 ACM Symposium on Virtual Reality Software and Technology;0;10.1145/1450579.1450625;We present Opportunistic Controls, a class of user interaction techniques for augmented reality (AR) applications that support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. Opportunistic Controls leverage characteristics of these affordances to provide passive haptics that ease gesture input, simplify gesture recognition, and provide tangible feedback to the user. 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons is mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of a user study in which participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique.;0;0;0;0;0;0;1;0;0
174;ubiGaze: ubiquitous augmented reality messaging using gaze gestures;"B\^{a}ce, Mihai and Lepp\""{a}nen, Teemu and de Gomez, David Gil and Gomez, Argenis Ramirez";2016;SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications;0;10.1145/2999508.2999530;We describe ubiGaze, a novel wearable ubiquitous method to augment any real-world object with invisible messages through gaze gestures that lock the message into the object. This enables a context and location dependent messaging service, which users can utilize discreetly and effortlessly. Further, gaze gestures can be used as an authentication method, even when the augmented object is publicly known. We developed a prototype using two wearable devices: a Pupil eye tracker equipped with a scene camera and a Sony Smartwatch 3. The eye tracker follows the users' gaze, the scene camera captures distinct features from the selected real-world object, and the smartwatch provides both input and output modalities for selecting and displaying messages. We describe the concept, design, and implementation of our real-world system. Finally, we discuss research implications and address future work.;0;0;0;0;0;0;1;0;0
176;ARnatomy: tangible AR app for learning gross anatomy;Seo, Jinsil Hwaryoung and Storey, James and Chavez, John and Reyna, Diana and Suh, Jinkyo and Pine, Michelle;2014;ACM SIGGRAPH 2014 Posters;0;10.1145/2614217.2614279;"Our goal is to develop a Tangible Augmented-Reality Interface
for mobile devices to enhance the effectiveness of learning gross
anatomy in a group and/or individual study settings. Learning
anatomy is fundamental to every health profession and related
domains (Dance Science and Visual Art). However, many
students spend most of their time memorizing anatomical terms
shown in 2D in a textbook without learning or understanding the
spatial relationships. This is problematic in that the students do
not completely grasp the relevance of the material, and therefore
rapidly loose what they have memorized. It is believed that
cadaver dissection is the optimal method of anatomy education
[Winkelmann, 2007]. Cadaver dissection provides knowledge of
the shape and size of the organs, bones, and muscles. In addition,
it gives students a spatial appreciation on how individual body
parts are positioned relative to the rest of the body. However it has
limited accessibility beyond a lab setting. It is also getting
difficult because of an increased recognition in animal rights
issues. This prototype merges the textbook information on skeletal
structures and the tactile interaction of physical bones. To
accomplish this goal, we created a prototype that can recognize a
variety of 3D print bones with visual markers. Once recognized
the bones are populated with virtual text labels that move on the
screen to match the video camera feed of the bones.";0;0;0;0;0;0;1;0;0
182;Wearable interaction and home automation to improve the patient wellness: study of perceived benefits;de la Gu\'{\i}a, Elena and L\'{o}pez, Vicente and Olivares, Teresa and Orozco, Luis and Lozano, Mar\'{\i}a D. and Penichet, Victor;2020;Proceedings of the 5th Workshop on ICTs for Improving Patients Rehabilitation Research Techniques;0;10.1145/3364138.3364162;"The concept of Health Smart House aims at giving an autonomous life, in their own home, to people who suffering from a chronic disease, older, handicapped people, etc. However, developing a smart health system for home is a challenge. A long-term, it must meet the specific needs of any user. In addition, it must provide the most important daily services. That is, to design and develop smart health system is important to know patients' needs and the point of view of caregivers, doctors, nurses, etc. In this paper, we have based on a smart-home environment that use home automation and tangible and wearable interaction to improve the patients' quality of life. In order to know the point of view the patient, caregivers, doctors, etc. We conducted an expert survey to identify challenges and advantages in home automation and system interaction. The survey is divided into three sections: the patients' load cognitive using the system; the most useful variables related to data gathering through sensors, and the system social acceptance. Thanks to the results obtained, we contribute to a better understanding about important factors in healthcare, home automation and user interaction.";0;0;0;0;0;0;1;0;0
184;PIPLEX: tangible experience in an augmented reality video game;Blanco, Jos\'{e} Mar\'{\i}a and Landry, Pascal and C., Sebasti\'{a}n Mealla and Mazzone, Emanuela and Par\'{e}s, Narc\'{\i}s;2010;Proceedings of the 9th International Conference on Interaction Design and Children;0;10.1145/1810543.1810590;In this paper we describe a work in progress of a mixed-reality framework based on tangible interface applied to a video game designed for children. This video game, called PIPLEX, lays on the ability of the users to solve a puzzle through modelling malleable materials (namely plasticine and cardboard). We explain the implementation of PIPLEX, its interaction rules and the physical set-up. Additionally, we suggest future applications that can be developed in the context of our framework.;0;0;0;0;0;0;1;0;0
186;CheckMate: Exploring a Tangible Augmented Reality Interface for Remote Interaction;"G\""{u}nther, Sebastian and M\""{u}ller, Florian and Schmitz, Martin and Riemann, Jan and Dezfuli, Niloofar and Funk, Markus and Sch\""{o}n, Dominik and M\""{u}hlh\""{a}user, Max";2018;Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems;0;10.1145/3170427.3188647;The digitalized world comes with increasing Internet capabilities, allowing to connect persons over distance easier than ever before. Video conferencing and similar online applications create great benefits bringing people who physically cannot spend as much time as they want virtually together. However, such remote experiences can also tend to lose the feeling of traditional experiences. People lack direct visual presence and no haptic feedback is available. In this paper, we tackle this problem by introducing our system called CheckMate. We combine Augmented Reality and capacitive 3D printed objects that can be sensed on an interactive surface to enable remote interaction while providing the same tangible experience as in co-located scenarios. As a proof-of-concept, we implemented a sample application based on the traditional chess game.;0;0;0;0;0;0;1;0;0
188;Closing the Distance: Mixed and Augmented Reality, Tangibles and Indigenous Culture Preservation;"Sieck, J\""{u}rgen and Zaman, Tariq";2017;Proceedings of the Ninth International Conference on Information and Communication Technologies and Development;0;10.1145/3136560.3136586;Since the beginning of the 21st century, computer technologies are more and more able to understand the real world by the informational structure of meta-data. With the advent of new tools, the traditional distinction between culture and technology has become obsolete. In this paper, we will discuss different approaches to create mixed reality applications for Indigenous cultural preservation and also best practices of developing augmented and mixed reality systems in two different cultural contexts. We will describe several technical aspects of mobile devices, tangible user interfaces and context-sensitive services in information systems for museums and local communities developed at the researchers' affiliated institutions. Our initial evaluations show that the systems are engaging and encourage intergenerational knowledge exchange, thus have the potential to help in cultural preservation of partner communities.;0;0;0;0;0;0;1;0;0
189;Opportunistic interaction in the challenged internet of things;"Wirtz, Hanno and R\""{u}th, Jan and Serror, Martin and Bitsch Link, J\'{o} \'{A}gila and Wehrle, Klaus";2014;Proceedings of the 9th ACM MobiCom Workshop on Challenged Networks;0;10.1145/2645672.2645679;Under intermittent Internet connectivity, enabling interaction between smart objects and mobile users in the Internet of Things (IoT) becomes a challenge. We thus discuss the notion of a 'Challenged IoT' and propose Direct Interaction with Smart Challenged Objects (DISCO), enabling objects to define their interaction patterns and interface. Building on the distinct features of Bluetooth Low Energy (BLE), objects then convey their interface directly to mobile users. DISCO mitigates the need for Internet connectivity and pre-installed interfaces, i.e., smartphone apps, of existing approaches and proposes autonomous and local interaction with smart objects as a challenged network scenario. We implement DISCO for Android and iOS smartphones as well as Linux and Arduino objects and illustrate the design space of interaction patterns with Augmented Reality (AR) interaction based on visual object recognition within the tangible interaction sphere of the user. Our system evaluation shows the immediate real-life feasibility and applicability of DISCO on current hardware.;0;0;0;0;0;0;1;0;0
199;Physikit: Data Engagement Through Physical Ambient Visualizations in the Home;Houben, Steven and Golsteijn, Connie and Gallacher, Sarah and Johnson, Rose and Bakker, Saskia and Marquardt, Nicolai and Capra, Licia and Rogers, Yvonne;2016;Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems;0;10.1145/2858036.2858059;"Internet of things (IoT) devices and sensor kits have the potential to democratize the access, use, and appropriation of data. Despite the increased availability of low cost sensors, most of the produced data is ""black box"" in nature: users often do not know how to access or interpret data. We propose a ""human-data design"" approach in which end-users are given tools to create, share, and use data through tangible and physical visualizations. This paper introduces Physikit, a system designed to allow users to explore and engage with environmental data through physical ambient visualizations. We report on the design and implementation of Physikit, and present a two-week field study which showed that participants got an increased sense of the meaning of data, embellished and appropriated the basic visualizations to make them blend into their homes, and used the visualizations as a probe for community engagement and social behavior. [yes and no - check plant, fig 11]";0;0;0;0;0;0;1;0;0
200;ATSI: Augmented and Tangible Sonic Interaction;Pugliese, Roberto and Politis, Archontis and Takala, Tapio;2015;Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction;0;10.1145/2677199.2680550;This paper presents ATSI, a system for sonic augmentation of physical objects with spatialized sounds and their control by gestural interaction. The implementation combines tracking of the users' hands and head with commodity hardware, and binaural spatialized sound rendering over headphones. The user may attach sounds to common objects and the system maintains the correct spatial auditory perspective inside the augmented scene during the actions of attaching sounds, picking and moving the sounding objects, or exploring the scene. Attention has been given to the binaural rendering of distance cues to support the above actions with perceptual realism in small scale environments and for many objects. Through a user study assessing the localization accuracy that can be achieved with the system, we show that sound rendering performance looks appropriate for applications such as auditory displays for context and object specific information, sonic design for architectural planning and interior design, and music applications.;0;0;0;0;0;0;1;0;0
208;Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices;Ledo, David and Anderson, Fraser and Schmidt, Ryan and Oehlberg, Lora and Greenberg, Saul and Grossman, Tovi;2017;Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems;0;10.1145/3025453.3025652;"Interactive, smart objects-customized to individuals and uses-are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects' ""smarts"". Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.";0;0;0;0;0;0;1;0;0
209;Paper Gaming: Creating IoT Paper Interactions with Conductive Inks and Web-connectivity through EKKO;Lochrie, Mark and Mills, John and Egglestone, Paul and Skelly, Martin;2015;Proceedings of the 2015 Annual Symposium on Computer-Human Interaction in Play;0;10.1145/2793107.2810321;"Paper is ubiquitous. It forms a substantial part of our everyday activities and interactions; ranging from our take-away coffee cups -- to wallpaper -- to rail tickets -- to board and card games. Imagine if you could connect paper to the Internet, interact and update it with additional data but without recourse to reprinting or using e-ink alternatives. This paper explores work examining conductive inks and web-connectivity of printed objects, which form part of an emergent sub-field within the Internet of Things (IoT) and paper. Our research is starting to explore a range of media uses, such as interactive newspapers, books, beer mats and now gaming environments through prototype IoT device named EKKO; a clip that allows conductive ink frameworks to detect human touch interaction revealing rich media content through a mobile application as the 'second screen'.";0;0;0;0;0;0;1;0;0
211;FlexTouch: Enabling Large-Scale Interaction Sensing Beyond Touchscreens Using Flexible and Conductive Materials;Wang, Yuntao and Zhou, Jianyu and Li, Hanchuan and Zhang, Tengxiang and Gao, Minxuan and Cheng, Zhuolin and Yu, Chun and Patel, Shwetak and Shi, Yuanchun;2019;Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.;0;10.1145/3351267;In this paper, we present FlexTouch, a technique that enables large-scale interaction sensing beyond the spatial constraints of capacitive touchscreens using passive low-cost conductive materials. This is achieved by customizing 2D circuit-like patterns with an array of conductive strips that can be easily attached to the sensing nodes on the edge of the touchscreen. FlexTouch requires no hardware modification, and is compatible with various conductive materials (copper foil tape, silver nanoparticle ink, ITO frames, and carbon paint), as well as fabrication methods (cutting, coating, and ink-jet printing). Through a series of studies and illustrative examples, we demonstrate that FlexTouch can support long-range touch sensing for up to 4 meters and everyday object presence detection for up to 2 meters. Finally, we show the versatility and feasibility of FlexTouch through applications such as body posture recognition, human-object interaction as well as enhanced fitness training experiences.;0;0;0;0;0;0;1;0;0
213;An intravenous injection simulator using augmented reality for veterinary education and its evaluation;Lee, Jun and Kim, WonJong and Seo, Anna and Jun, JiSun and Lee, SeungYeon and Kim, Jee-In and Eom, KiDong and Pyeon, Muwook and Lee, Hanku;2012;Proceedings of the 11th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry;0;10.1145/2407516.2407524;This paper proposes a simulator for veterinary education based on augmented reality. We selected an intravenous injection procedure for the simulation because the injection procedure is the most frequently used procedure during veterinary training and the most difficult stage for beginning veterinary students. The proposed AR simulator provides with a tangible prop, of which shape looks like a leg of a real dog. It also has an injection simulator, which receives user's input and sends force feedbacks to indicate results of the injection simulation. We developed a Workbench type AR system with an LED display and cameras for visual information processing. Finally, we evaluated its performance through experiments and user studies to check its acceptance level and usability of the proposed system. We compared the proposed system with a VR based system using a monitor. The results showed that the proposed system showed better performances over these systems.;0;0;0;0;0;0;1;0;0
227;Understanding Family Collaboration Around Lightweight Modification of Everyday Objects in the Home;Williams, Kristin and Pulivarthy, Rajitha and Hudson, Scott E. and Hammer, Jessica;2019;Proc. ACM Hum.-Comput. Interact.;0;10.1145/3359287;The internet-of-things (IoT) carries substantial costs by urging households to replace their possessions with new, internet connected versions of everyday objects. Beyond financial, these costs include waste, work to arrange and orchestrate objects to suit households, and that of acquiring new skills. Upcycling domestic objects could offer households greater discretion and control over these costs by supporting the ability to tailor IoT to the home. To understand how households might do this, we conducted a home study with 10 diverse American households over 7 days to surface the approaches families are likely to use when tailoring IoT to their existing possessions. We asked family members to enact their process using endowed sticker props---IoT Stickers---to modify objects in their home. We develop a framework of how families make light weight modifications of domestic possessions, summarize trends of their object modifications, and describe the burdens such a system could impose.;0;0;0;0;0;0;1;0;0
232;Annexing Reality: Enabling Opportunistic Use of Everyday Objects as Tangible Proxies in Augmented Reality;Hettiarachchi, Anuruddha and Wigdor, Daniel;2016;Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems;0;10.1145/2858036.2858134;"Advances in display and tracking technologies hold the promise of increasingly immersive augmented-reality experiences. Unfortunately, the on-demand generation of haptic experiences is lagging behind these advances in other feedback channels. We present Annexing Reality; a system that opportunistically annexes physical objects from a user's current physical environment to provide the best-available haptic sensation for virtual objects. It allows content creators to a priori specify haptic experiences that adapt to the user's current setting. The system continuously scans user's surrounding, selects physical objects that are similar to given virtual objects, and overlays the virtual models on to selected physical ones reducing the visual-haptic mismatch. We describe the developer's experience with the Annexing Reality system and the techniques utilized in realizing it. We also present results of a developer study that validates the usability and utility of our method of defining haptic experiences.";0;0;0;0;0;0;1;0;0
240;TouchCounters: designing interactive electronic labels for physical containers;Yarin, Paul and Ishii, Hiroshi;2000;CHI '00 Extended Abstracts on Human Factors in Computing Systems;0;10.1145/633292.633305;TouchCounters is a system of electronic labels, physical storage containers, and shelving surfaces linked by specialized hardware. The labels record and display accumulated usage information directly upon physical storage containers, thus allowing access to this information during the performance of physical tasks. A distributed communications network allows remote access to this data from the Internet. This video demonstrates the functionality of TouchCounters in supporting the shared use of physical resources.;0;0;0;0;0;0;1;0;0
241;"Touch &amp; Share: RFID based ubiquitous file containers";S\'{a}nchez, Iv\'{a}n and Riekki, Jukka and Rousu, Jarkko and Pirttikangas, Susanna;2008;Proceedings of the 7th International Conference on Mobile and Ubiquitous Multimedia;0;10.1145/1543137.1543148;"Here, we present Touch &amp; Share, an application for sharing multimedia content in our everyday environment. Our main design criterion is ease of use. Visual icons placed in the environment act as data storage (file containers). Users can drop multimedia files into the containers and pick files from them by touching the icons with their mobile terminals. The icons are placed on, or near, the physical objects the files are related to. RFID tags are placed behind the icons and they store metadata information about the files available in the corresponding containers. When users bring their terminals near the icons, the terminals' RFID readers communicate with the tags, read the contents of a container from the tag and possibly write information about new files. The actual files are stored in a server, but the user experiences the files to be in the containers. The main contributions of this work are a general application for storing multimedia content in nearly any place or everyday object and a fully functional prototype built using only commercial off-the-self technology.";0;0;0;0;0;0;1;0;0
250;LightBeam: nomadic pico projector interaction with real world objects;"Huber, Jochen and Steimle, J\""{u}rgen and Liao, Chunyuan and Liu, Qiong and M\""{u}hlh\""{a}user, Max";2012;CHI '12 Extended Abstracts on Human Factors in Computing Systems;0;10.1145/2212776.2223828;Pico projectors have lately been investigated as mobile display and interaction devices. We propose to use them as 'light beams': Everyday objects sojourning in a beam are turned into dedicated projection surfaces and tangible interaction devices. While this has been explored for large projectors, the affordances of pico projectors are fundamentally different: they have a very small and strictly limited projection ray and can be carried around in a nomadic way during the day. Thus it is unclear how this could be actually leveraged for tangible interaction with physical, real world objects. We have investigated this in an exploratory field study and contribute the results. Based upon these, we present exemplary interaction techniques and early user feedback.;0;0;0;0;0;0;1;0;0
253;StretchAR: Exploiting Touch and Stretch as a Method of Interaction for Smart Glasses Using Wearable Straps;Paredes, Luis and Ipsita, Ananya and Mesa, Juan C. and Martinez Garrido, Ramses V. and Ramani, Karthik;2022;Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.;0;10.1145/3550305;Over the past decade, augmented reality (AR) developers have explored a variety of approaches to allow users to interact with the information displayed on smart glasses and head-mounted displays (HMDs). Current interaction modalities such as mid-air gestures, voice commands, or hand-held controllers provide a limited range of interactions with the virtual content. Additionally, these modalities can also be exhausting, uncomfortable, obtrusive, and socially awkward. There is a need to introduce comfortable interaction techniques for smart glasses and HMDS without the need for visual attention. This paper presents StretchAR, wearable straps that exploit touch and stretch as input modalities to interact with the virtual content displayed on smart glasses. StretchAR straps are thin, lightweight, and can be attached to existing garments to enhance users' interactions in AR. StretchAR straps can withstand strains up to 190% while remaining sensitive to touch inputs. The strap allows the effective combination of these inputs as a mode of interaction with the content displayed through AR widgets, maps, menus, social media, and Internet of Things (IoT) devices. Furthermore, we conducted a user study with 15 participants to determine the potential implications of the use of StretchAR as input modalities when placed on four different body locations (head, chest, forearm, and wrist). This study reveals that StretchAR can be used as an efficient and convenient input modality for smart glasses with a 96% accuracy. Additionally, we provide a collection of 28 interactions enabled by the simultaneous touch-stretch capabilities of StretchAR. Finally, we facilitate recommendation guidelines for the design, fabrication, placement, and possible applications of StretchAR as an interaction modality for AR content displayed on smart glasses.;0;0;0;0;0;0;1;0;0
264;WebClip: a connector for ubiquitous physical input and output for touch screen devices;Kubitza, Thomas and Pohl, Norman and Dingler, Tilman and Schmidt, Albrecht;2013;Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing;0;10.1145/2493432.2493520;It has become extremely easy for developers to build custom software for smartphone and tablet computers. However, it is still hard to extend those devices with external electronics, e.g. additional sensors and actuators. In the moment when external hardware can be easily attached to phones and tablets a wide new application space will be opened up. With WebClip we present a device offering digital and analogue I/O ports that can be controlled and monitored by just clipping the device onto a capacitive touch screen. A web page in the browser of the touch screen device is used to control the bi-directional communication. Data from the WebClip to the device is sent by emulating touches on the screen whereas the reverse direction uses light sensors on the bottom side of the clip to receive light sequences emitted by the web page. A simple Javascript API is offered to build custom web applications. We have successfully tested our prototype with a variety of phones and tablet computers and report on performance and limitations.;0;0;0;0;0;0;1;0;0
286;Pmomo: Projection Mapping on Movable 3D Object;Zhou, Yi and Xiao, Shuangjiu and Tang, Ning and Wei, Zhiyong and Chen, Xu;2016;Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems;0;10.1145/2858036.2858329;We introduce Pmomo (acronym of projection mapping on movable object), a dynamic projection mapping system that tracks the 6-DOF position of real-world object, and shades it with virtual 3D contents by projection. The system can precisely lock the projection on the moving object in real-time, even the one with complex geometry. Based on depth camera, we developed a novel and robust tracking method that samples the structure of the object into low-density point cloud, then performs an adaptive searching scheme for the registration procedure. As a fully interactive system, our method can handle both internal and external complex occlusions, and can quickly track back the object even when losing track. In order to further improve the realism of the projected virtual textures, our system innovatively culls occlusions away from projection, which is achieved by a facet-covering method. As a result, the Pmomo system enables the possibility of new interactive Augmented Reality applications that require high-quality dynamic projection effect.;0;0;0;0;0;0;1;0;0
310;TouchCounters: designing interactive electronic labels for physical containers;Yarin, Paul and Ishii, Hiroshi;1999;Proceedings of the SIGCHI Conference on Human Factors in Computing Systems;0;10.1145/302979.303110;"We present TouchCounters, an integrated system of electronic
modules, physical storage containers, and shelving surfaces for the
support of collaborative physical work. Through physical sensors
and local displays, TouchCounters record and display usage history
information upon physical storage containers, thus allowing access
to this information during the performance of real-world tasks. A
distributed communications network allows this data to be exchanged
with a server, such that users can access this information from
remote locations as well.Based upon prior work in ubiquitous computing and tangible
interfaces, TouchCounters incorporate new techniques, including
usage history tracking for physical objects and multi-display
visualization. This paper describes the components, interactions,
implementation, and conceptual approach of the TouchCounters
system. [duplicate?]";0;0;0;0;0;0;1;0;0
311;Tribo Tribe: Triboelectric Interaction Sensing with 3D Physical Interfaces;Liu, Xin and Han, Bo and Zheng, Clement and Yen, Ching Chiuan;2022;Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems;0;10.1145/3491101.3519759;This research introduces “Tribo Tribe”, a technique for fabricating 3D tangible interactive interfaces capable of sensing movement inputs through ubiquitous materials. Tribo Tribe is built on the working principle of Triboelectric Nanogenerators (TENG) to enable self-powered sensing to 3D systems. We introduce a tool kit that facilitates designers and makers to easily customise both prototyping and sensing through TENG technology. We also demonstrate four design possibilities for different fields to illustrate how Tribo Tribe can instrument TENG into 3D physical interactive prototypes (Figure 1).;0;0;0;0;0;0;1;0;0
322;TouchSound: making sounds with everyday objects;Peng, Huaishu;2010;Proceedings of the Fifth International Conference on Tangible, Embedded, and Embodied Interaction;0;10.1145/1935701.1935820;In this paper I am presenting TouchSound, a wearable device that generates sounds and music by interacting with everyday objects by hand. By mixing tactile and auditory sensations, TouchSound promotes enjoyable musical experiences. It could be used by musicians to explore new ways to perform music, for educators to teach music to kids, and provide an opportunity for people to play music by touching everyday objects.;0;0;0;0;0;0;1;0;0
323;Fillables: everyday vessels as tangible controllers with adjustable haptics;Corsten, Christian and Wacharamanotham, Chat and Borchers, Jan;2013;CHI '13 Extended Abstracts on Human Factors in Computing Systems;0;10.1145/2468356.2468732;We introduce Fillables: low-cost and ubiquitous everyday vessels that are appropriated as tangible controllers whose haptics are tuned ad-hoc by filling, e.g., with water. We show how Fillables can assist users in video navigation and drawing tasks with physical controllers whose adjustable output granularity harmonizes with their haptic feedback. As proof of concept, we implemented a drawing application that uses vessels to control a virtual brush whose stroke width corresponds to the filling level. Furthermore, we found that humans can distinguish nine levels of haptic feedback when sliding water-filled paper cups (300 ml capacity) over a wooden surface. This discrimination follows Weber's Law and was facilitated by sloshing of water.;0;0;0;0;0;0;1;0;0
327;LumiTouch: an emotional communication device;Chang, Angela and Resner, Ben and Koerner, Brad and Wang, XingChen and Ishii, Hiroshi;2001;CHI '01 Extended Abstracts on Human Factors in Computing Systems;0;10.1145/634067.634252;We present the Lumitouch system consisting of a pair of interactive picture frames. When one user touches her picture frame, the other picture frame lights up. This touch is translated to light over an Internet connection. We introduce a semi-ambient display that can transition seamlessly from periphery to foreground in addition to communicating emotional content. In addition to enhancing the communication between loved ones, people can use LumiTouch to develop a personal emotional language.Based upon prior work on telepresence and tangible interfaces, LumiTouch explores emotional communication in tangible form. This paper describes the components, interactions, implementation and design approach of the LumiTouch system.;0;0;0;0;0;0;1;0;0
331;KeyStub: A Passive RFID-based Keypad Interface Using Resonant Stubs;Nolan, John and Qian, Kun and Zhang, Xinyu;2024;Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.;0;10.1145/3631442;The proliferation of the Internet of Things is calling for new modalities that enable human interaction with smart objects. Recent research has explored RFID tags as passive sensors to detect finger touch. However, existing approaches either rely on custom-built RFID readers or are limited to pre-trained finger-swiping gestures. In this paper, we introduce KeyStub, which can discriminate multiple discrete keystrokes on an RFID tag. KeyStub interfaces with commodity RFID ICs with multiple microwave-band resonant stubs as keys. Each stub's geometry is designed to create a predefined impedance mismatch to the RFID IC upon a keystroke, which in turn translates into a known amplitude and phase shift, remotely detectable by an RFID reader. KeyStub combines two ICs' signals through a single common-mode antenna and performs differential detection to evade the need for calibration and ensure reliability in heavy multi-path environments. Our experiments using a commercial-off-the-shelf RFID reader and ICs show that up to 8 buttons can be detected and decoded with accuracy greater than 95%. KeyStub points towards a novel way of using resonant stubs to augment RF antenna structures, thus enabling new passive wireless interaction modalities.;0;0;0;0;0;0;1;0;0
334;SurfaceSight: A New Spin on Touch, User, and Object Sensing for IoT Experiences;Laput, Gierad and Harrison, Chris;2019;Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems;0;10.1145/3290605.3300559;"IoT appliances are gaining consumer traction, from smart thermostats to smart speakers. These devices generally have limited user interfaces, most often small buttons and touchscreens, or rely on voice control. Further, these devices know little about their surroundings unaware of objects, people and activities happening around them. Consequently, interactions with these ""smart"" devices can be cumbersome and limited. We describe SurfaceSight, an approach that enriches IoT experiences with rich touch and object sensing, offering a complementary input channel and increased contextual awareness. For sensing, we incorporate LIDAR into the base of IoT devices, providing an expansive, ad hoc plane of sensing just above the surface on which devices rest. We can recognize and track a wide array of objects, including finger input and hand gestures. We can also track people and estimate which way they are facing. We evaluate the accuracy of these new capabilities and illustrate how they can be used to power novel and contextually-aware interactive experiences.";0;0;0;0;0;0;1;0;0
342;Bottles as a minimal interface to access digital information;Ishii, Hiroshi and Mazalek, Ali and Lee, Jay;2001;CHI '01 Extended Abstracts on Human Factors in Computing Systems;0;10.1145/634067.634180;"We present the design of a minimal interface to access digital information using glass bottles as ""containers"" and ""controls"". The project illustrates our attempt to explore the transparency of an interface that weaves itself into the fabric of everyday life, and exploits the emotional aspects of glass bottles that are both tangible and visual. This paper describes the design of the bottle interface, and the implementation of the musicBottles installation, in which the opening of each bottle releases the sound of a specific instrument.";0;0;0;0;0;0;1;0;0
506;Investigating Universal Appliance Control through Wearable Augmented Reality;"Becker, Vincent and Rauchenstein, Felix and S\""{o}r\""{o}s, G\'{a}bor";2019;Proceedings of the 10th Augmented Human International Conference 2019;0;10.1145/3311823.3311853;The number of interconnected devices around us is constantly growing. However, it may become challenging to control all these devices when control interfaces are distributed over mechanical elements, apps, and configuration webpages. We investigate interaction methods for smart devices in augmented reality. The physical objects are augmented with interaction widgets, which are generated on demand and represent the connected devices along with their adjustable parameters. For example, a loudspeaker can be overlaid with a controller widget for its volume. We explore three ways of manipulating the virtual widgets: (a) in-air finger pinching and sliding, (b) whole arm gestures rotating and waving, (c) incorporating physical objects in the surrounding and mapping their movements to the interaction primitives. We compare these methods in a user study with 25 participants and find significant differences in the preference of the users, the speed of executing commands, and the granularity of the type of control.;0;0;0;0;0;0;1;0;0
518;Turning everyday objects into passive tangible controllers;Drogemuller, Adam and Walsh, James and Smith, Ross T. and Adcock, Matt and Thomas, Bruce H;2021;Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction;0;10.1145/3430524.3442460;In augmented reality (AR), gesture/hand-based interactions are becoming more common place over tangible user interfaces (TUIs) and physical controllers. Major concerns regard portability and battery-life with TUIs and physical controllers in an AR environment. For a TUI or physical controller to be usable ”on the go” and in the field, they need to be compact such that they can be easily deployable. Subsequently, it is desirable to be low-energy or powerless, such that the device remains functional after long periods. In this paper, we present our initial design towards a powerless, passive controller that leverages the optical camera of a AR head-mounted display (such as the Hololens or Magic Leap) for tracking. We discuss related work, design motivations, high and low fidelity designs, opportunistic/adaptable input modalities, and use cases. We conclude with propositions for future work.;0;0;0;0;0;0;1;0;0
589;Affordance-Based and User-Defined Gestures for Spatial Tangible Interaction;Gong, Weilun and Santosa, Stephanie and Grossman, Tovi and Glueck, Michael and Clarke, Daniel and Lai, Frances;2023;Proceedings of the 2023 ACM Designing Interactive Systems Conference;0;10.1145/3563657.3596032;Although mid-air hand gestures have been widely adopted by VR/AR products (e.g., Quest 2 and HoloLens), some drawbacks remain due to their lack of tangibility and tactile feedback. Opportunistic Tangible User Interfaces could address these shortcomings by repurposing existing objects in one's physical environment. However, there has yet to be a systematic investigation of the gestures that would be desirable when using opportunistic objects or how such gestures would be impacted by such objects. In this work, we conducted an elicitation study to investigate the desirability of object and gesture combinations across a variety of interactions. The results contribute (1) an opportunistic tangible UI gesture set for spatial interfaces, and (2) an Affordance-Based Object Selector Scheme that identifies ideal objects for tangible input given a desired input gesture, based on that object's physical affordances. Arising from these findings is the vision of the Adaptive Tangible User Interface, which supports the on-the-fly composition of tangible interfaces based on the affordances found in the physical environment and a user's input task.;0;0;0;0;0;0;1;0;0
1343;Opportunistic Tangible User Interfaces for Augmented Reality;Henderson, Steven and Feiner, Steven;2010;IEEE Transactions on Visualization and Computer Graphics;0;10.1109/TVCG.2009.91;Opportunistic Controls are a class of user interaction techniques that we have developed for augmented reality (AR) applications to support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. By leveraging characteristics of these affordances to provide passive haptics that ease gesture input, Opportunistic Controls simplify gesture recognition, and provide tangible feedback to the user. In this approach, 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons can be mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of two user studies. In the first, participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique. In the second, participants proposed and demonstrated user interfaces incorporating Opportunistic Controls for two domains, allowing us to gain additional insights into how user interfaces featuring Opportunistic Controls might be designed.;0;0;0;0;0;0;1;0;0
1409;User-Defined Interaction Using Everyday Objects for Augmented Reality First Person Action Games;Greenslade, Mac and Clark, Adrian and Lukosch, Stephan;2022;2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW);0;10.1109/VRW55335.2022.00272;In this paper, we present an elicitation study to explore how people use everyday objects for augmented reality first person games. 24 participants were asked to select items from a range of everyday objects to use as controllers for three different classes of virtual object. Participants completed tasks using their selected items and rated the experience using the Augmented Reality Immersion (ARI) questionnaire. Results indicate no strong consensus linking any specific everyday object to any virtual object across our testing population. Based on these findings, we recommend developers provide the ability for users to choose the everyday objects they prefer.;0;0;0;0;0;0;1;0;0
1459;Save the Space Elevator: An Escape Room Scenario Involving Passive Haptics in Mixed Reality;Davari, Shakiba and Li, Yuan and Lisle, Lee and Lu, Feiyu and Zhang, Lei and Blustein, Leslie and Feng, Xueting and Gabaldon, Brianna and Kwiatkowski, Marc and Bowman, Doug A.;2019;2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR);0;10.1109/VR.2019.8798051;This paper presents our solution to the 2019 3DUI Contest challenge, which focuses on passive haptics. We aimed to provide a compelling user experience in virtual reality while overcoming the limitations of physical space and current tracking devices. To meet these goals, we designed a time travel scenario that incorporates several novel features. A time machine increases efficiency in the use of physical space. A passive haptic camera prop provides a help system that is integrated into the storyline. Finally, the concept of a “temporal stabilizer” provides a plausible way to reuse a single tracking device to track multiple passive haptic props.;0;0;0;0;0;0;1;0;0
1460;A Collaborative Virtual Reality Escape Room with Passive Haptics;Hanus, Austin and Hoover, Mindy and Lim, Alex and Miller, Jack;2019;2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR);0;10.1109/VR.2019.8798241;Escape rooms have recently become a popular way to socialize and problem solve in an immersive environment. However, it can be difficult and expensive to create elaborate escape rooms with realistic props. Virtual reality (VR) technology allows developers to create and customize escape rooms more easily. However, to truly make the VR escape room immersive, physical and cooperative interactions are necessary. In this paper, the authors propose and demonstrate a two-player VR escape room developed for the HTC Vive. Physical interactions were made possible by using passive haptics in the form of simple props tracked using HTC Vive trackers and controllers. Additionally, the HTC Vives were networked, and players hands were tracked using the Leap Motion to provide head and hand position cues to teammates.;0;0;0;0;0;0;1;0;0
1462;Crystal Palace: Merging Virtual Objects and Physical Hand-held Tools;Kashiwagi, Toshiro and Sumi, Kaoru and Fels, Sidney and Zhou, Qian and Wu, Fan;2019;2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR);0;10.1109/VR.2019.8798370;We developed a mixed-reality approach, called Crystal Palace, which provides prop-based passive haptic feedback for the 3DUI contest. In this system, we propose an interface of controlling virtual object with physical tools such as scissors, a spray and a screwdriver as game controllers. Our prop-based approach provides natural visual affordance and passive haptics by associating physical tools with various 3D tasks. Using 360 degrees spherical display, players will be able to use common real-world tools in an intuitive way to interact with 3D objects in a virtual escape room.;0;0;0;0;0;0;1;0;0
1475;Wormholes in VR: Teleporting Hands for Flexible Passive Haptics;Ban, Reigo and Matsumoto, Keigo and Narumi, Takuji and Kuzuoka, Hideaki;2022;2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR);0;10.1109/ISMAR55827.2022.00093;Presenting haptic feedback in virtual reality (VR) is a long-standing challenge, with passive haptics being one way of presenting haptic feedback inexpensively. However, passive haptics requires props in physical environments that are co-located with their virtual counterparts, which is often not the case in the real world. Although redirected hands and other methods have previously been proposed to solve this problem, significant differences between the displayed and actual hand positions can cause the degradation of presence and sense of embodiment, limiting the range of presentable environments. In this study, we present a new hand displacement method called wormholes, in which the virtual hand is teleported discontinuously as the user inserts their hand into the hole. The experiment showed that the wormhole could maintain the sense of embodiment, presence, and task performance even with large hand displacements. Our method enables to apply passive haptics even when the actual and virtual environments are quite different, contributing to the realization of inexpensive and flexible haptic presentation in VR applications.;0;0;0;0;0;0;1;0;0
1476;Beyond the Tyranny of the Pixel: Exploring the Physicality of Information Visualization;Moere, Andrew Vande;2008;2008 12th International Conference Information Visualisation;0;10.1109/IV.2008.84;This paper consists of a review of contemporary methods that map and materialize abstract data as physical artifacts. With computing technology and the access of information influencing every aspect of our everyday lives, one can question the current habit of information displays to dasiasimulatepsila real world metaphors, and whether information could instead be conveyed by approximating the analogue and tangible characteristics of our daily experiences. This paper introduces five different degrees of dasiadata physicalitypsila, which differ in the level of abstraction of how data is mapped and perceived by human senses: ambient display, pixel sculptures, object augmentation, data sculptures and alternative modality. This categorization demonstrates the potential of information visualization as a communication medium in its own right, which proliferates beyond the ubiquitous pixel-based, light-emitting surfaces of today.;0;0;0;0;0;0;1;0;0
1482;Kuka: An Architecture for Associating an Augmented Artefact with Its User Using Wearable Sensors;Fujinami, Kaori and Pirttikangas, Susanna;2008;2008 IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing (sutc 2008);0;10.1109/SUTC.2008.42;In this paper, we present an architecture for associating a person and an artefact (a daily object) utilized by him/her in an ad-hoc and extensible manner. The proximity of the acceleration signal patterns from wearable sensors on different parts of the body and sensor augmented artefacts are utilized for making an association. The proposed architecture also provides a filtering facility to reduce the possibility of miss-association between an artefact and a person who is using a different artefact in a similar way but just exists in close vicinity. With the association information, a system can provide a more reliable context-aware service as the particular role of an artefact and the proprietary nature of such an accessory gives additional knowledge for the system. A distributed architecture and a correlation coefficient based approach is realized. We describe the design, a prototype implementation and a basic experiment.;0;0;0;0;0;0;1;0;0
1491;Using the Visuo-Haptic Illusion to Perceive and Manipulate Different Virtual Objects in Augmented Reality;Zhang, Li and He, Weiping and Hu, Yupeng and Wang, Shuxia and Bai, Huidong and Billinghurst, Mark;2021;IEEE Access;0;10.1109/ACCESS.2021.3121390;The prop-based 3D virtual object manipulation method is widely used for interaction in Augmented Reality (AR) due to its convenience and flexibility. However, when the represented virtual object is different from the physical prop, the look and feel of the object are not well aligned. To address this problem, we present a dynamic finger remapping approach to creating a visuo-haptic illusion that dynamically adjusts the presented virtual hand posture to fit different sizes and shapes of virtual objects in AR. The finger movement toward a physical prop is synchronously remapped to the movement of the virtual fingers towards the corresponding virtual object. We developed a system that enables users to perceive consistent visual and tactile feedback while grasping and releasing various virtual objects represented by a physical prop. We conducted a user study to explore the effect of this visuo-haptic illusion on the perceived size of virtual objects, setting the sizes of the rendered virtual object and the physical prop as independent variables. We found that the perceived size of a virtual object varied with its rendered size in an almost linear fashion, while the physical prop size did not significantly affect the perception. We also conducted a second study to compare our system with a current prop-based method on virtual object manipulation. The results indicated that the remapped hands could effectively improve the realism and naturalness of the experience.;0;0;0;0;0;0;1;0;0
3993;Interactive Space Generation through Play Exploring Form Creation and the Role of Simulation on the Design Table;Schieck, Ava Fatah Gen. and Penn, Alan and Mottram, Chiron and Strothmann, Andreas and Ohlenburg, Jan and Broll, Wolfgang and Aish, Francis;2004;Interactive Space Generation through Play Exploring Form Creation and the Role of Simulation on the Design Table;0;https://discovery.ucl.ac.uk/id/eprint/126/1/eCAADe2004.pdf;In this paper we report on recent developments in ARTHUR: an approach to support complex design and planning decisions for architects together with the simulation of pedestrian movement and the integration of existing CAD tools on the design table. Following a brief introduction, past and current work that has taken a similar approach will be reviewed. Next we describe a scenario that integrates agent-based simulations of pedestrian movement with space creation, and then give an overview of the system before finally discussing findings related to recent user evaluation studies of the system. This paper suggests that the integration of simulated pedestrian movement on the design table, while going through a cycle of reflection-in-action, plays a vital role in exploring possible design solutions and encourages new and different ways of thinking about design problems. © 2004, Education and research in Computer Aided Architectural Design in Europe. All rights reserved.;0;0;0;0;0;0;1;0;0
