title;authors;year;publicationtitle;citationcount;doi;abstract;population;intervention;comparison;outcomes;context;checklater;include
A Hybrid Method for Implicit Intention Inference Based on Punished-Weighted Naïve Bayes;Zheng Gao,Shiqian Wu,Zhonghua Wan,Sos Agaian,;2023;IEEE Transactions on Neural Systems and Rehabilitation Engineering;0;https://doi.org/10.1109/TNSRE.2023.3259550;Gaze-based implicit intention inference provides a new human-robot interaction for people with disabilities to accomplish activities of daily living independently. Existing gaze-based intention inference is mainly implemented by the data-driven method without prior object information in intention expression, which yields low inference accuracy. Aiming to improve the inference accuracy, we propose a gaze-based hybrid method by integrating model-driven and data-driven intention inference tailored to disability applications. Specifically, intention is considered as the combination of verbs and nouns. The objects corresponding to the nouns are regarded as intention-interpreting objects and served as prior knowledge, i.e., punished factors. The punished factor considers the object information, i.e., the priority in object selection. Class-specific attribute weighted naïve Bayes model learned through training data is presented to represent the relationship among intentions and objects. An intention inference engine is developed by combining the human prior knowledge, and the data-driven class-specific attribute weighted naïve Bayes model. Computer simulations: (i) verify the contribution of each critical component of the proposed model, (ii) evaluate the inference accuracy of the proposed model, and (iii) show that the proposed method is superior to state-of-the-art intention inference methods in terms of accuracy.;0;0;0;0;0;0;0
A Lack of Restraint: Comparing Virtual Reality Interaction Techniques for Constrained Transport Seating;Graham Wilson,Mark McGill,Daniel Medeiros,Stephen Brewster,;2023;IEEE Transactions on Visualization and Computer Graphics;0;https://doi.org/10.1109/TVCG.2023.3247084;Standalone Virtual Reality (VR) headsets can be used when travelling in cars, trains and planes. However, the constrained spaces around transport seating can leave users with little physical space in which to interact using their hands or controllers, and can increase the risk of invading other passengers' personal space or hitting nearby objects and surfaces. This hinders transport VR users from using most commercial VR applications, which are designed for unobstructed 1-2m 360° home spaces. In this paper, we investigated whether three at-a-distance interaction techniques from the literature could be adapted to support common commercial VR movement inputs and so equalise the interaction capabilities of at-home and on-transport users: Linear Gain, Gaze-Supported Remote Hand, and AlphaCursor. First, we analysed commercial VR experiences to identify the most common movement inputs so that we could create gamified tasks based on them. We then investigated how well each technique could support these inputs from a constrained $50\mathrm{x}50\text{cm}$ space (representative of an economy plane seat) through a user study $(\mathrm{N}=16)$, where participants played all three games with each technique. We measured task performance, unsafe movements (play boundary violations, total arm movement) and subjective experience and compared results to a control ‘at-home’ condition (with unconstrained movement) to determine how similar performance and experience were. Results showed that Linear Gain was the best technique, with similar performance and user experience to the ‘at-home’ condition, albeit at the expense of a high number of boundary violations and large arm movements. In contrast, AlphaCursor kept users within bounds and minimised arm movement, but suffered from poorer performance and experience. Based on the results, we provide eight guidelines for the use of, and research into, at-a-distance techniques and constrained spaces.;0;0;0;0;0;0;0
A Novel Integrated Eye-Tracking System With Stereo Stimuli for 3-D Gaze Estimation;Jinglin Sun,Zhipeng Wu,Han Wang,Peiguang Jing,Yu Liu,;2023;IEEE Transactions on Instrumentation and Measurement;0;https://doi.org/10.1109/TIM.2022.3225009;Eye-tracking technology is increasingly used in applications such as 3-D displays and human–computer interaction (HCI). However, most current eye trackers focus on 2-D point-of-gaze (PoG) estimation and cannot provide accurate gaze depth. Concerning future applications such as HCI with 3-D displays, we propose an integrated binocular eye-tracking device with stereo stimuli to provide highly accurate 3-D PoG estimation. In our device, the 3-D stereo imaging system can provide users with a friendly and immersive 3-D visual experience without wearing any accessories. The eye-capturing system can directly record the users’ eye movements under 3-D stimuli without disturbance, which can realize front views of eye images while ensuring the user’s wide field of view (FOV). A regression-based 3-D eye-tracking model is built based on collected eye movement data under stereo stimuli. Our model estimates users’ 2-D gaze with features defined by eye region landmarks and further estimates 3-D gaze with a multisource feature set constructed by comprehensive eye movement features and disparity features from stereo stimuli. Two test stereo scenes with different depths of field are designed to verify the model’s effectiveness. Experimental results of fourfold cross-validation show that the average error for 2-D gaze estimation was  $0.\mathrm{65}^{\circ} $ . For 3-D gaze estimation, the average errors are 1.69 cm over the workspace volume 50  $\times $  30  $\times $  75 cm3 and 0.14 m over the workspace volume 2.4  $\times $  4.0  $\times $  7.9  $\text{m}^{3}$ . Two preliminary validation experiments also demonstrate that our system has broad application prospects, such as 3-D vision technologies and medical applications related to neurological diseases.;0;0;0;0;0;0;0
A Review of EEG-Based User Authentication: Trends and Future Research Directions;Christos A. Fidas,Dimitrios Lyras,;2023;IEEE Access;0;https://doi.org/10.1109/ACCESS.2023.3253026;"Recently, the use of Electroencephalography (EEG) in scientific research on User Authentication (UA) has led to cutting-edge experiments that seek to identify and authenticate individuals based on their brain activity in particular usage scenarios. Utilizing EEG signals, derived from brain activity, might provide innovative solutions to contemporary security issues in traditional knowledge-based user authentication, including the threat of shoulder surfing. In this review paper, we analyze 108 different EEG-based user authentication experiments based on the following perspectives: a) the user experimental setup, with an emphasis on the applied EEG- protocols; b) the artificial intelligence techniques employed and finally c) the security and privacy preservation aspects. The reviewed papers cover a broad time frame from 1998 to 2022 and include various experimental protocols and algorithms used for classifying EEG signals. Additionally, the majority of the referenced works report findings from multiple experiments that incorporate distinct approaches and configurations. This leads to a discussion on best practices for EEG-based User Authentication and conclusions suggesting future research directions that consists, among others, of considering homomorphically encrypted biometric templates for information leakage prevention through federated learning approaches in decentralized architectures. We anticipate that the present literature review will provide a roadmap for future research by considering efficiently and effective EEG-based User Authentication methods while at the same time preserving privacy.";0;0;0;0;0;0;0
A Survey on Remote Assistance and Training in Mixed Reality Environments;Catarina G. Fidalgo,Yukang Yan,Hyunsung Cho,Maurício Sousa,David Lindlbauer,Joaquim Jorge,;2023;IEEE Transactions on Visualization and Computer Graphics;0;https://doi.org/10.1109/TVCG.2023.3247081;The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a taxonomy based on degree of collaboration, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. All supplemental materials are available at https://augmented-perception.org/publications/2023-training-survey.html.;0;0;0;0;0;0;0
An Effective Head-Based HRI for 6D Robotic Grasping Using Mixed Reality;Chengjie Zhang,Chengyu Lin,Yuquan Leng,Zezheng Fu,Yaoyu Cheng,Chenglong Fu,;2023;IEEE Robotics and Automation Letters;0;https://doi.org/10.1109/LRA.2023.3261701;People with two-arm disabilities need to grasp various densely placed objects in their daily life. However, current arm-free human-robot interfaces (HRIs), such as language-based and gaze-based HRI, are difficult to effectively control the robotic arm to complete the above task. To achieve this task effectively, we innovatively propose an arm-free HRI based on Mixed Reality (MR) feedback and head control, which enables people to give full play to their intelligence in correcting robot perception errors and determining grasp decision-making. MR feedback makes 3D visualization of robot perception results and virtual grippers aligned with real grippers. Head control ensures these disabled people can flexibly and effectively control the gripper's 6D pose. In experiments, we densely place 15 objects of different sizes and shapes from daily life, including transparent and specular objects, such as a measuring tape, transparent bottles, and a charging head, which are grasped by 10 subjects using our HRI. Our experiment results and comparison results show that our HRI can more effectively complete the task, has the adaptability for unseen object grasping, and has high point cloud error tolerance.;0;0;0;0;0;0;0
Automated Eye Movement Classification Based on EMG of EOM Signals Using FBSE-EWT Technique;Sibghatullah Inayatullah Khan,Ram Bilas Pachori,;2023;IEEE Transactions on Human-Machine Systems;0;https://doi.org/10.1109/THMS.2023.3238113;The accurate automated eye movement classification is gaining importance in the field of human–computer interaction (HCI). The present article aims at the classification of six types of eye movements from electromyogram (EMG) of extraocular muscles (EOM) signals using the Fourier–Bessel series expansion-based empirical wavelet transform (FBSE-EWT) with time and frequency-domain (TAFD) features. The FBSE-EWT of EMG signals results in Fourier–Bessel intrinsic mode functions (FBIMFs), which correspond to the frequency contents in the signal. A hybrid approach is used to select the prominent FBIMFs followed by the statistical and signal complexity-based feature extraction. Furthermore, metaheuristic optimization algorithms are employed to reduce the feature space dimension. The discrimination ability of the reduced feature set is verified by Kruskal–Wallis statistical test. Multiclass support vector machine (MSVM) has been employed for classification. First, the classification has been performed with TAFD features followed by the combination of TAFD and FBSE-EWT-based reduced feature set. The combination of TAFD and FBSE-EWT-based feature set has provided good classification performance. This study demonstrates the efficacy of FBSE-EWT and subsequent metaheuristic feature selection algorithms in classifying the eye movements from EMG of EOM signals. The combination of TAFD and the selected features through salp swarm optimization algorithm has provided maximum classification accuracy of 98.91% with MSVM employing Gaussian and radial basis function kernels. Thus, the proposed approach has the potential to be used in HCI applications involving biomedical signals.;0;0;0;0;0;0;0
Brain Network Reorganization During Visual Search Task Revealed by a Network Analysis of Fixation-Related Potential;Linze Qian,Xianliang Ge,Zhao Feng,Sujie Wang,Jingjia Yuan,Yunxian Pan,Hongqi Shi,Jie Xu,Yu Sun,;2023;IEEE Transactions on Neural Systems and Rehabilitation Engineering;1;https://doi.org/10.1109/TNSRE.2023.3242771;Visual search is ubiquitous in daily life and has attracted substantial research interest over the past decades. Although accumulating evidence has suggested complex neurocognitive processes underlying visual search, the neural communication across the brain regions remains poorly understood. The present work aimed to fill this gap by investigating functional networks of fixation-related potential (FRP) during the visual search task. Multi-frequency electroencephalogram (EEG) networks were constructed from 70 university students (male/female = 35/35) using FRPs time-locked to target and non-target fixation onsets, which were determined by concurrent eye-tracking data. Then graph theoretical analysis (GTA) and a data-driven classification framework were employed to quantitatively reveal the divergent reorganization between target and non-target FRPs. We found distinct network architectures between target and non-target mainly in the delta and theta bands. More importantly, we achieved a classification accuracy of 92.74% for target and non-target discrimination using both global and nodal network features. In line with the results of GTA, we found that the integration corresponding to target and non-target FRPs significantly differed, while the nodal features contributing most to classification performance primarily resided in the occipital and parietal-temporal areas. Interestingly, we revealed that females exhibited significantly higher local efficiency in delta band when focusing on the search task. In summary, these results provide some of the first quantitative insights into the underlying brain interaction patterns during the visual search process.;0;0;0;0;0;0;0
CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation Dataset for 360$^\circ$ VR Videos;Tong Xue,Abdallah El Ali,Tianyi Zhang,Gangyi Ding,Pablo Cesar,;2023;IEEE Transactions on Multimedia;6;https://doi.org/10.1109/TMM.2021.3124080;"Watching 360$^\circ$ videos using Virtual Reality (VR) head-mounted displays (HMDs) provides interactive and immersive experiences, where videos can evoke different emotions. Existing emotion self-report techniques within VR however are either retrospective or interrupt the immersive experience. To address this, we introduce the Continuous Physiological and Behavioral Emotion Annotation Dataset for 360$^\circ$ Videos (CEAP-360VR). We conducted a controlled study (N=32) where participants used a Vive Pro Eye HMD to watch eight validated affective 360$^\circ$ video clips, and annotated their valence and arousal (V-A) continuously. We collected (a) behavioral (head and eye movements; pupillometry) signals (b) physiological (heart rate, skin temperature, electrodermal activity) responses (c) momentary emotion self-reports (d) within-VR discrete emotion ratings (e) motion sickness, presence, and workload. We show the consistency of continuous annotation trajectories and verify their mean V-A annotations. We find high consistency between viewed 360$^\circ$ video regions across subjects, with higher consistency for eye than head movements. We furthermore run baseline classification experiments, where Random Forest classifiers with 2s segments show good accuracies for subject-independent models: 66.80% (V) and 64.26% (A) for binary classification; 49.92% (V) and 52.20% (A) for 3-class classification. Our open dataset allows further experiments with continuous emotion self-reports collected in 360$^\circ$ VR environments, which can enable automatic assessment of immersive Quality of Experience (QoE) andmomentary affective states.";0;0;0;0;0;0;0
Camera Frame Misalignment in a Teleoperated Eye-in-Hand Robot: Effects and a Simple Correction Method;Liao Wu,Fangwen Yu,Thanh Nho Do,Jiaole Wang,;2023;IEEE Transactions on Human-Machine Systems;0;https://doi.org/10.1109/THMS.2022.3217453;Misalignment between the camera frame and the operator frame is commonly seen in a teleoperated system and usually degrades the operation performance. The effects of such misalignment have not been fully investigated for eye-in-hand systems–systems that have the camera (eye) mounted to the end-effector (hand) to gain compactness in confined spaces such as in endoscopic surgery. This article provides a systematic study on the effects of the camera frame misalignment in a teleoperated eye-in-hand robot and proposes a simple correction method in the view display. A simulation is designed to compare the effects of the misalignment under different conditions. Users are asked to move a rigid body from its initial position to the specified target position via teleoperation, with different levels of misalignment simulated. It is found that misalignment between the input motion and the output view is much more difficult to compensate by the operators when it is in the orthogonal direction ($\sim$40 s) compared with the opposite direction ($\sim$20 s). An experiment on a real concentric tube robot with an eye-in-hand configuration is also conducted. Users are asked to telemanipulate the robot to complete a pick-and-place task. Results show that with the correction enabled, there is a significant improvement in the operation performance in terms of completion time (mean 40.6%, median 38.6%), trajectory length (mean 34.3%, median 28.1%), difficulty (50.5%), unsteadiness (49.4%), and mental stress (60.9%).;0;0;0;0;0;0;0
Detection of Dyslexic Children Using Machine Learning and Multimodal Hindi Language Eye-Gaze-Assisted Learning System;Yogesh Kumar Meena,Hubert Cecotti,Braj Bhushan,Ashish Dutta,Girijesh Prasad,;2023;IEEE Transactions on Human-Machine Systems;0;https://doi.org/10.1109/THMS.2022.3221848;Children with dyslexia need specific instructions for spelling and word analysis from an early age. It is important to provide appropriate tools using technology for writing aids to such children that can help them to input text, while providing multiple feedback. However, it is unclear how children with dyslexia can efficiently use a gaze-based virtual keyboard (VK). In this study, we propose to use the typing performance of a multimodal Hindi language eye-gaze-assisted learning system based on a VK to help in the reduction of tracking errors for people with writing and reading deficiencies and to detect children with dyslexia. Performance was assessed at three levels: eye tracker, eye tracker with soft switch, and touchscreen as a baseline modality using a predefined copy-typing task. The system was validated through a series of experiments with 32 children (16 dyslexic and 16 control). The results show that the workload and the usability of the system are substantially different for children with dyslexia. Children with dyslexia have a lower typing performance when using the touchscreen modality or the eye tracker only. The detection of children with dyslexia from others was assessed with seven different types of classifiers using the typing speed on different words (AUC $>$ 0.9). These results highlight the need to have fully inclusive VKs. This work demonstrates the superior use of a multimodal system with participants having unique neuropsychological conditions and that the proposed system can be used to detect children with dyslexia.;0;0;0;0;0;0;0
Driver State Modeling Through Latent Variable State Space Framework in the Wild;Arash Tavakoli,Steven Boker,Arsalan Heydarian,;2023;IEEE Transactions on Intelligent Transportation Systems;0;https://doi.org/10.1109/TITS.2022.3221858;Analyzing the impact of the environment on drivers’ stress level and workload is of high importance for designing human-centered driver-vehicle interaction systems and to ultimately help build a safer driving experience. However, driver’s state, including stress level and workload, are latent variables that cannot be measured on their own and should be estimated through sensor measurements such as psychophysiological measures. We propose using a latent-variable state-space modeling framework for driver state analysis. By using latent-variable state-space models, we model drivers’ workload and stress levels as latent variables estimated through multimodal human sensing data, under the perturbations of the environment in a state-space format and in a holistic manner. Through using a case study of multimodal driving data collected from 11 participants, we first estimate the latent stress level and workload of drivers from their heart rate, gaze measures, and intensity of facial action units. We then show that external contextual elements such as the number of vehicles as a proxy for traffic density and secondary task demands may be associated with changes in driver’s stress levels and workload. We also show that different drivers may be impacted differently by the aforementioned perturbations. We found out that drivers’ latent states at previous timesteps are highly associated with their current states. Additionally, we discuss the utility of state-space models in analyzing the possible lag between the two latent variables of stress level and workload, which might be indicative of information transmission between the different parts of the driver’s psychophysiology in the wild.;0;0;0;0;0;0;0
ECA Application Design: Augmented reality combined with eye-tracking technology based on Mobile device for myopia prevention;Tianshi Xie,Cheryl Seals,;2023;2023 IEEE International Conference on Consumer Electronics (ICCE);0;https://doi.org/10.1109/ICCE56470.2023.10043476;In daily life, with the popularity of mobile devices such as mobile phones and iPad, it is more and more common for children and adolescents to see myopia caused by long-term viewing of mobile device screens or failing to maintain a healthy visual distance. In response to this phenomenon and problem, we designed an intelligent eye care assistance system (ECA) based on mobile devices, which integrates web browsers and utilizes AR technology combined with depth camera technology. Compared with the current methods of controlling distance through ultrasonic sensors, infrared sensing devices, etc., the ECA system is more convenient to use and does not require any additional hardware equipment. The system implements the method of controlling the distance between the user and the mobile device screen through three steps: real-time eye tracking, distance measurement, and reminder for the user. And for the problem of users' long-term viewing of mobile device screens, we added a time reminder control function in the system design to better prevent myopia and protect users' eyesight. This paper proposes a common framework for an intelligent eye-protection ECA system to prevent various ophthalmic diseases [1] caused by short viewing distance watching mobile devices screens, including cataracts, glaucoma, retinal detachment, and myopic macular degeneration.;0;0;0;0;0;0;0
EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive Virtual Reality;Zhiming Hu,Andreas Bulling,Sheng Li,Guoping Wang,;2023;IEEE Transactions on Visualization and Computer Graphics;1;https://doi.org/10.1109/TVCG.2021.3138902;Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering. However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks. Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements. We first collect eye and head movements of 30 participants performing four tasks, i.e., Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos. Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination. We then propose EHTask – a novel learning-based method that employs eye and head movements to recognize user tasks in VR. We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of $84.4\%$84.4% versus $62.8\%$62.8%) and on a real-world dataset ($61.9\%$61.9% versus $44.1\%$44.1%). As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.;0;0;0;0;0;0;0
Estimation of Drivers’ Gaze Behavior by Potential Attention When Using Human–Machine Interface;Yu Yamaguchi,Yuki Okafuji,Takahiro Wada,Kazuomi Murakami,Hiroyuki Ishida,;2023;IEEE Access;0;https://doi.org/10.1109/ACCESS.2022.3192859;Recently, various visual information presentation systems known as human–machine interfaces (HMIs), such as road projection lamp systems, have been developed for safe driving. However, it is unclear how these HMIs change the drivers’ gaze behavior and improve their cognitive awareness of the environment. Therefore, in this study, we introduce the concept of potential attention to propose a probabilistic method to estimate drivers’ gaze behavior when using HMIs. The potential attention hypothesis can propose an explanation to understand gaze behavior. This method assigns potential attention to objects the driver is likely to gaze, such as vehicles and pedestrians, thereby estimating the driver’s potential gaze point from potential attentions. The study is divided into two steps. The first step analyzes the drivers’ gaze behavior in the simulator experiment when a road projection lamp is displayed to alert pedestrians. In the second step, we propose a method for estimating the driver’s gaze through the potential attention method based on the results of the simulator experiment. The modeling results for gaze behavior measured in the simulator experiment as the first step show that gaze behavior can be estimated with high accuracy. This proposed method is expected to apply to a method to determine where the HMI display should be placed.;0;0;0;0;0;0;0
Extended Control With Hybrid Gaze-BCI for Multi-Robot System Under Hands-Occupied Dual-Tasking;Hong Zeng,Yitao Shen,Dengfeng Sun,Xuhui Hu,Pengcheng Wen,Aiguo Song,;2023;IEEE Transactions on Neural Systems and Rehabilitation Engineering;0;https://doi.org/10.1109/TNSRE.2023.3234971;Currently there still remains a critical need of human involvements for multi-robot system (MRS) to successfully perform their missions in real-world applications, and the hand-controller has been commonly used for the operator to input MRS control commands. However, in more challenging scenarios involving concurrent MRS control and system monitoring tasks, where the operator’s both hands are busy, the hand-controller alone is inadequate for effective human-MRS interaction. To this end, our study takes a first step toward a multimodal interface by extending the hand-controller with a hands-free input based on gaze and brain-computer interface (BCI), i.e., a hybrid gaze-BCI. Specifically, the velocity control function is still designated to the hand-controller that excels at inputting continuous velocity commands for MRS, while the formation control function is realized with a more intuitive hybrid gaze-BCI, rather than with the hand-controller via a less natural mapping. In a dual-task experimental paradigm that simulated the hands-occupied manipulation condition in real-world applications, operators achieved improved performance for controlling simulated MRS (average formation inputting accuracy increases 3%, average finishing time decreases 5 s), reduced cognitive load (average reaction time for secondary task decreases 0.32 s) and perceived workload (average rating score decreases 15.84) with the hand-controller extended by the hybrid gaze-BCI, over those with the hand-controller alone. These findings reveal the potential of the hands-free hybrid gaze-BCI to extend the traditional manual MRS input devices for creating a more operator-friendly interface, in challenging hands-occupied dual-tasking scenarios.;0;0;0;0;0;0;0
Game-Based Social Interaction Platform for Cognitive Assessment of Autism Using Eye Tracking;Yi-Ling Chien,Chia-Hsin Lee,Yen-Nan Chiu,Wen-Che Tsai,Yuan-Che Min,Yang-Min Lin,Jui-Shen Wong,Yi-Li Tseng,;2023;IEEE Transactions on Neural Systems and Rehabilitation Engineering;0;https://doi.org/10.1109/TNSRE.2022.3232369;"The design goals of recently developed serious games are to improve attention, affective recognition, and social interactions among individuals with autism. However, most previous studies on serious games used behavioral questionnaires to evaluate their effectiveness. The cognitive assessment of individuals with autism after behavioral intervention or drug treatment has become important because it provides promising biomarkers to assess improvement after cognitive intervention. In this study, we developed a game-based social interaction platform incorporating an eye-tracking system for children and preadolescents with autism. Three modules (focusing on gaze following, facial emotion recognition, and social interaction skills) are included in the platform; participants with autism learn these according to their cognitive abilities. The eye-tracking results showed decreased fixation durations when autistic children looked at positive emotional expressions and focused on multiple targets. Prolonged saccade durations and shorter fixation times for social-related facial emotion expressions were also found in preadolescents and teenagers with autism. Our findings suggest that these atypical gaze patterns are reliable biomarkers for evaluating the social and cognitive functions of autistic individuals while playing serious games. The proposed platform’s game-based modules and the findings regarding aberrant gaze patterns in autistic individuals demonstrate the possibility of evaluating cognitive functions and intervention effectiveness by using eye-tracking signals in a serious game or real-life environment.";0;0;0;0;0;0;0
Gaze movement operability and sense of spatial presence assessment while operating a robot avatar;Kaoruko Shinkawa,Yoshihiro Nakata,;2023;2023 IEEE/SICE International Symposium on System Integration (SII);0;https://doi.org/10.1109/SII55687.2023.10039342;"The utilization of teleoperated robots has great potential for expanding the range of human social activities, and there has been much anticipation for this technology in recent years. To use teleoperated robots as human surrogates, it is essential to develop a teleoperation system that provides the operator with a high level of presence. Conventional robot avatars are generally operated by synchronizing the head with the operator. However, eye movements are essential for human gaze, and conventional operation (i.e., head movement synchronization only) may reduce perceived presence. In this study, we used a robot avatar that could move its head and eyeballs. We examined the effects of the synchronization of eye movements with the operator on the operability of the robot and the presence perceived by the operator. We conducted an experiment in which participants performed a task where they looked at an object under two conditions: head or head-eye synchronization between the participant and robot avatar. We investigated the effects of eye movement synchronization on task performance, spatial presence, and workload. The results showed that synchronization of eye movements improved operability and reduced workload; however, it did not improve presence. This study suggests that synchronization of eye movements may make the teleoperation of an avatar robot more comfortable and provides meaningful insights into research approaches for the improving presence of an avatar robot.";0;0;0;0;0;0;0
Improving Intention Detection in Single-Trial Classification Through Fusion of EEG and Eye-Tracker Data;Xianliang Ge,Yunxian Pan,Sujie Wang,Linze Qian,Jingjia Yuan,Jie Xu,Nitish Thakor,Yu Sun,;2023;IEEE Transactions on Human-Machine Systems;1;https://doi.org/10.1109/THMS.2022.3225633;Intention decoding is an indispensable procedure in hands-free human–computer interaction (HCI). A conventional eye-tracker system using a single-model fixation duration may issue commands that ignore users' real expectations. Here, an eye-brain hybrid brain–computer interface (BCI) interaction system was introduced for intention detection through the fusion of multimodal eye-tracker and event-related potential (ERP) [a measurement derived from electroencephalography (EEG)] features. Eye-tracking and EEG data were recorded from 64 healthy participants as they performed a 40-min customized free search task of a fixed target icon among 25 icons. The corresponding fixation duration of eye tracking and ERP were extracted. Five previously-validated linear discriminant analysis (LDA)-based classifiers [including regularized LDA, stepwise LDA, Bayesian LDA, shrinkage linear discriminant analysis (SKLDA), and spatial-temporal discriminant analysis] and the widely-used convolutional neural network (CNN) method were adopted to verify the efficacy of feature fusion from both offline and pseudo-online analysis, and the optimal approach was evaluated by modulating the training set and system response duration. Our study demonstrated that the input of multimodal eye tracking and ERP features achieved a superior performance of intention detection in the single-trial classification of active search tasks. Compared with the single-model ERP feature, this new strategy also induced congruent accuracy across classifiers. Moreover, in comparison with other classification methods, we found that SKLDA exhibited a superior performance when fusing features in offline tests (ACC = 0.8783, AUC = 0.9004) and online simulations with various sample amounts and duration lengths. In summary, this study revealed a novel and effective approach for intention classification using an eye-brain hybrid BCI and further supported the real-life application of hands-free HCI in a more precise and stable manner.;0;0;0;0;0;0;0
Improving Multi-Agent Trajectory Prediction Using Traffic States on Interactive Driving Scenarios;Chalavadi Vishnu,Vineel Abhinav,Debaditya Roy,C. Krishna Mohan,Ch. Sobhan Babu,;2023;IEEE Robotics and Automation Letters;0;https://doi.org/10.1109/LRA.2023.3258685;Predicting trajectories of multiple agents in interactive driving scenarios such as intersections, and roundabouts are challenging due to the high density of agents, varying speeds, and environmental obstacles. Existing approaches use relative distance and semantic maps of intersections to improve trajectory prediction. However, drivers base their driving decision on the overall traffic state of the intersection and the surrounding vehicles. So, we propose to use traffic states that denote changing spatio-temporal interaction between neighboring vehicles, to improve trajectory prediction. An example of a traffic state is a clump state which denotes that the vehicles are moving close to each other, i.e., congestion is forming. We develop three prediction models with different architectures, namely, Transformer-based (TS-Transformer), Generative Adversarial Network-based (TS-GAN), and Conditional Variational Autoencoder-based (TS-CVAE). We show that traffic state-based models consistently predict better future trajectories than the vanilla models. TS-Transformer produces state-of-the-art results on two challenging interactive trajectory prediction datasets, namely, Eye-on-Traffic (EOT), and INTERACTION. Our qualitative analysis shows that traffic state-based models have better aligned trajectories to the ground truth.;0;0;0;0;0;0;0
Leveling the Playing Field: A Comparative Reevaluation of Unmodified Eye Tracking as an Input and Interaction Modality for VR;Ajoy S. Fernandes,T. Scott Murdison,Michael J. Proulx,;2023;IEEE Transactions on Visualization and Computer Graphics;0;https://doi.org/10.1109/TVCG.2023.3247058;In this study, we establish a much-needed baseline for evaluating eye tracking interactions using an eye tracking enabled Meta Quest 2 VR headset with 30 participants. Each participant went through 1098 targets using multiple conditions representative of AR/VR targeting and selecting tasks, including both traditional standards and those more aligned with AR/VR interactions today. We use circular white world-locked targets, and an eye tracking system with sub-1-degree mean accuracy errors running at approximately 90Hz. In a targeting and button press selection task, we, by design, compare completely unadjusted, cursor-less, eye tracking with controller and head tracking, which both had cursors. Across all inputs, we presented targets in a configuration similar to the ISO 9241–9 reciprocal selection task and another format with targets more evenly distributed near the center. Targets were laid out either flat on a plane or tangent to a sphere and rotated toward the user. Even though we intended this to be a baseline study, we see unmodified eye tracking, without any form of a cursor, or feedback, outperformed the head by 27.9% and performed comparably to the controller (5.63% decrease) in throughput. Eye tracking had improved subjective ratings relative to head in Ease of Use, Adoption, and Fatigue (66.4%, 89.8%, and 116.1 % improvements, respectively) and had similar ratings relative to the controller (reduction by 4.2%, 8.9%, and 5.2% respectively). Eye tracking had a higher miss percentage than controller and head (17.3% vs 4.7% vs 7.2% respectively). Collectively, the results of this baseline study serve as a strong indicator that eye tracking, with even minor sensible interaction design modifications, has tremendous potential in reshaping interactions in next-generation AR/VR head mounted displays.;0;0;0;0;0;0;0
Monitoring Engagement in Online Classes Through WiFi CSI;Vijay Kumar Singh,Pragma Kar,Ayush Madhan Sohini,Madhav Rangaiah,Sandip Chakraborty,Mukulika Maity,;2023;2023 15th International Conference on COMmunication Systems & NETworkS (COMSNETS);0;https://doi.org/10.1109/COMSNETS56262.2023.10041341;Due to the Covid-19 pandemic, people have been forced to move to online spaces to attend classes or meetings and so on. The effectiveness of online classes depends on the engagement level of students. A straightforward way to monitor the engagement is to observe students' facial expressions, eye gazes, head gesticulations, hand movements, and body movements through their video feed. However, video-based engagement detection has limitations, such as being influenced by video backgrounds, lighting conditions, camera angles, unwillingness to open the camera, etc. In this work, we propose a non-intrusive mechanism of estimating engagement level by monitoring the head gesticulations through channel state information (CSI) of WiFi signals. First, we conduct an anonymous survey to investigate whether the head gesticulation pattern is correlated with engagement. We then develop models to recognize head gesticulations through CSI. Later, we plan to correlate the head gesticulation pattern with the instructor's intent to estimate the students' engagement.;0;0;0;0;0;0;0
Non-Intrusive Real Time Eye Tracking Using Facial Alignment for Assistive Technologies;C. Leblond-Menard,S. Achiche,;2023;IEEE Transactions on Neural Systems and Rehabilitation Engineering;0;https://doi.org/10.1109/TNSRE.2023.3236886;"Most affordable eye tracking systems use either intrusive setup such as head-mounted cameras or use fixed cameras with infrared corneal reflections via illuminators. In the case of assistive technologies, using intrusive eye tracking systems can be a burden to wear for extended periods of time and infrared based solutions generally do not work in all environments, especially outside or inside if the sunlight reaches the space. Therefore, we propose an eye-tracking solution using state-of-the-art convolutional neural network face alignment algorithms that is both accurate and lightweight for assistive tasks such as selecting an object for use with assistive robotics arms. This solution uses a simple webcam for gaze and face position and pose estimation. We achieve a much faster computation time than the current state-of-the-art while maintaining comparable accuracy. This paves the way for accurate appearance-based gaze estimation even on mobile devices, giving an average error of around 4.5° on the MPIIGaze dataset (Zhang et al., 2019) and state-of-the-art average errors of 3.9° and 3.3° on the UTMultiview (Sugano et al., 2014) and GazeCapture (Krafka et al., 2016; Park et al., 2019) datasets respectively, while achieving a decrease in computation time of up to 91%.";0;0;0;0;0;0;0
Privacy-preserving datasets of eye-tracking samples with applications in XR;Brendan David-John,Kevin Butler,Eakta Jain,;2023;IEEE Transactions on Visualization and Computer Graphics;0;https://doi.org/10.1109/TVCG.2023.3247048;Virtual and mixed-reality (XR) technology has advanced significantly in the last few years and will enable the future of work, education, socialization, and entertainment. Eye-tracking data is required for supporting novel modes of interaction, animating virtual avatars, and implementing rendering or streaming optimizations. While eye tracking enables many beneficial applications in XR, it also introduces a risk to privacy by enabling re-identification of users. We applied privacy definitions of k-anonymity and plausible deniability (PD) to datasets of eye-tracking samples and evaluated them against the state-of-the-art differential privacy (DP) approach. Two VR datasets were processed to reduce identification rates while minimizing the impact on the performance of trained machine-learning models. Our results suggest that both PD and DP mechanisms produced practical privacy-utility trade-offs with respect to re-identification and activity classification accuracy, while k-anonymity performed best at retaining utility for gaze prediction.;0;0;0;0;0;0;0
RavenGaze: A Dataset for Gaze Estimation Leveraging Psychological Experiment Through Eye Tracker;Tao Xu,Bo Wu,Yuqiong Bai,Yun Zhou,;2023;2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG);0;https://doi.org/10.1109/FG57933.2023.10042793;One major challenge in appearance-based gaze estimation is the lack of high-quality labeled data. Establishing databases or datasets is a way to obtain accurate gaze data and test methods or tools. However, the methods of collecting data in existing databases are designed on artificial chasing target tasks or unintentional free-looking tasks, which are not natural and real eye interactions and cannot reflect the inner cognitive processes of humans. To fill this gap, we propose the first gaze estimation dataset collected from an actual psychological experiment by the eye tracker, called the RavenGaze dataset. We design an experiment employing Raven's Matrices as visual stimuli and collecting gaze data, facial videos as well as screen content videos simultaneously. Thirty-four participants were recruited. The results show that the existing algorithms perform well on our RavenGaze dataset in the 3D and 2D gaze estimation task, and demonstrate good generalization ability according to cross-dataset evaluation task. RavenGaze and the establishment of the benchmark lay the foundation for other researchers to do further in-depth research and test their methods or tools. Our dataset is available at https://intelligentinteractivelab.github.io/datasets/RavenGaze/index.html.;0;0;0;0;0;0;0
Review of Eye Tracking Metrics Involved in Emotional and Cognitive Processes;Vasileios Skaramagkas,Giorgos Giannakakis,Emmanouil Ktistakis,Dimitris Manousos,Ioannis Karatzanis,Nikolaos S. Tachos,Evanthia Tripoliti,Kostas Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis,;2023;IEEE Reviews in Biomedical Engineering;21;https://doi.org/10.1109/RBME.2021.3066072;Eye behaviour provides valuable information revealing one’s higher cognitive functions and state of affect. Although eye tracking is gaining ground in the research community, it is not yet a popular approach for the detection of emotional and cognitive states. In this paper, we present a review of eye and pupil tracking related metrics (such as gaze, fixations, saccades, blinks, pupil size variation, etc.) utilized towards the detection of emotional and cognitive processes, focusing on visual attention, emotional arousal and cognitive workload. Besides, we investigate their involvement as well as the computational recognition methods employed for the reliable emotional and cognitive assessment. The publicly available datasets employed in relevant research efforts were collected and their specifications and other pertinent details are described. The multimodal approaches which combine eye-tracking features with other modalities (e.g. biosignals), along with artificial intelligence and machine learning techniques were also surveyed in terms of their recognition/classification accuracy. The limitations, current open research problems and prospective future research directions were discussed for the usage of eye-tracking as the primary sensor modality. This study aims to comprehensively present the most robust and significant eye/pupil metrics based on available literature towards the development of a robust emotional or cognitive computational model.;0;0;0;0;0;0;0
State-of-the-Art in Smart Contact Lenses for Human–Machine Interaction;Yuanjie Xia,Mohamed Khamis,F. Anibal Fernandez,Hadi Heidari,Haider Butt,Zubair Ahmed,Tim Wilkinson,Rami Ghannam,;2023;IEEE Transactions on Human-Machine Systems;0;https://doi.org/10.1109/THMS.2022.3224683;Contact lenses have traditionally been used for vision correction applications. Recent advances in microelectronics and nanofabrication on flexible substrates have now enabled sensors, circuits, and other essential components to be integrated on a small contact lens platform. This has opened up the possibility of using contact lenses for a range of human–machine interaction (HMI) applications, including vision assistance, eye tracking, displays, and healthcare. In this article, we systematically review the range of smart contact lens materials, device architectures, and components that facilitate this interaction for different applications. In fact, evidence from our systematic review demonstrates that these lenses can be used to display information, detect eye movements, restore vision, and detect certain biomarkers in tear fluid. Consequently, whereas previous state-of the-art reviews in contact lenses focused exclusively on biosensing, our systematic review covers a wider range of smart contact lens applications in HMI. Moreover, we present a new method of classifying the literature on smart contact lenses according to their six constituent building blocks, which are the sensing, energy management, driver electronics, communications, substrate, and the input/output interfacing modules. Based on recent developments in each of these categories, we speculate the challenges and opportunities of smart contact lenses for HMI. Moreover, based on our analysis of the state-of-the-art, we develop guidelines for the future design of a self-powered smart contact lens concept with integrated energy harvesters, sensors, and communications modules. Therefore, our review is a critical evaluation of current data and is presented with the aim of guiding researchers to new research directions in smart contact lenses.;0;0;0;0;0;0;0
Static Laser Feedback Interferometry-Based Gaze Estimation for Wearable Glasses;Johannes Meyer,Stefan Gering,Enkelejda Kasneci,;2023;IEEE Sensors Journal;0;https://doi.org/10.1109/JSEN.2023.3250714;Fast and robust gaze estimation is a key technology for wearable glasses, as it enables novel methods of user interaction as well as display enhancement applications, such as foveated rendering. State-of-the-art video-based systems lack a high update rate, integrateability, slippage robustness, and low power consumption. To overcome these limitations, we propose a model-based fusion algorithm to estimate gaze from multiple static laser feedback interferometry (LFI) sensors, which are capable of measuring distance toward the eye and the eye’s rotational velocity. The proposed system is ambient light robust and robust to glasses slippage. During evaluation, a gaze accuracy of 1.79° at an outstanding update rate of 1 kHz is achieved, while the sensors consume only a fraction of the power compared with the state-of-the-art video-based system.;0;0;0;0;0;0;0
‘I Did Digital Tidying up for a More Adult Stage of Life’: Ritualistic Technology Appropriations During Life Transitions;Sara Wolf,Frauke Mörike,Diana Löffler,Jörn Hurtienne,;2023;Interacting with Computers;0;https://doi.org/10.1093/iwc/iwad001;Life transitions, such as the transition from childhood to adulthood, are often accompanied by meaning-making actions such as rituals. Rituals increasingly involve the use of interactive technology. While previous research has focused on specific contexts or technologies, a bird's eye view of the many appropriation styles during life transitions is missing. To identify the range of technology's appropriations, we analysed stories from 84 participants and compared these across different life transitions and technologies. We identified three roles interactive technology can play during life transitions: the role of (i) a facilitator easing the accomplishment of tasks within life transitions, (ii) an enabler creating opportunities for new transition rituals and (iii) a social actor that itself is the trigger or the content of transition rituals. We propose the three roles as a classification scheme to structure existing and future research and reflect on the design challenges and evaluation approaches.;0;0;0;0;0;0;0
